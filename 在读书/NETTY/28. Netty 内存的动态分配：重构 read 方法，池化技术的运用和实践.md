大家好，这一章的知识又要和内存分配的知识联系起来了，当然，这一章的重点肯定不是讲解内存分配的过程了，因为涉及到了服务端和客户端接收数据，所以肯定要使用到 ByteBuf 对象来当作字节消息的载体，所以，肯定会从程序内部的 PoolChunk 中申请堆外内存。本章就会围绕着内存分配的动态调整来讲解。比如说，被服务端管理的 NioSocketChannel 接收到客户端发来的消息了，第一次接收到了非常多的消息，需要申请 1 KB 的堆外内存来存放字节消息，所以程序内部就给用户分配了一个 1 KB 大小的 ByteBuf 对象；但紧接着第二次接收到的消息非常少，只需要 64 个字节就能存放。这时候再分配 1 KB 显然十分浪费，所以才有了动态分配内存的功能。当然，这一功能显然是针对 NioSocketChannel 的，服务端的 NioServerSocketChannel 只负责接收客户端连接，并不接收字节消息，因此本章讲解的知识自然也会分成两部分，一部分围绕着 NioServerSocketChannel 接收客户端连接来讲解，另一部分围绕 NioSocketChannel 接收字节消息来讲解。但是这两部分又可以总的概括一下，那就是重构 Channel 的 read 方法。这就是本章要讲解的全部内容了。

**重构 NioServerSocketChannel 的 read 方法**

在重构 NioServerSocketChannel 的 read 方法之前，让我们先简单回顾一下，在过去的代码版本中，当有客户端访问服务端，希望建立连接时，NioServerSocketChannel 是怎么做的。这就要再次回顾 NioEventLoop 了，因为对 IO 事件的判断和后续处理都是在那个类中进行的。当服务端接收到客户端连接后，就会触发 OP_ACCEPT 事件。请看下面代码块。

java

复制代码

`prite void processSelectedKey(SelectionKey k,AbstractNioChannel ch) throws Exception {     try {         //获取Unsafe类         final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe();         //得到key感兴趣的事件         int ops = k.interestOps();         //如果是连接事件         if (ops == SelectionKey.OP_CONNECT) {             ops &= ~SelectionKey.OP_CONNECT;             //重新把感兴趣的事件注册一下             k.interestOps(ops);             //这里要做真正的客户端连接处理             unsafe.finishConnect();         }         if (ops ==  SelectionKey.OP_READ) {             unsafe.read();         }         //接收到连接事件，说明有客户端连接进来了         if (ops == SelectionKey.OP_ACCEPT) {             unsafe.read();         }     } catch (CancelledKeyException ignored) {         throw new RuntimeException(ignored.getMessage());     } }`

可以看到，在上面代码块的第 25 行，如果服务端接收到客户端连接了，就会触发 OP_ACCEPT 事件，这时候就会调用 Unsafe 对象的 read 方法。而这个 read 方法最终又会调用到 Channel 中。确切地说，是会调用到 Channel 接口的内部 Unsafe 接口中的 read 方法。下面是该方法重构之前的实现逻辑。请看下面代码块。

java

复制代码

`@Override public void read() {     //该方法要在netty的线程执行器中执行     assert eventLoop().inEventLoop(Thread.currentThread());     //得到ChannelPipeline     final ChannelPipeline pipeline = pipeline();     Throwable exception = null;     try {         do {             //创建客户端的连接，存放在集合中             int localRead = doReadMessages(readBuf);             //返回值为0表示没有连接，直接退出即可             if (localRead == 0) {                 break;             }         } while (true);     } catch (Throwable t) {         exception = t;     }     int size = readBuf.size();     for (int i = 0; i < size; i ++) {         readPending = false;         //把每一个客户端的channel注册到工作线程上，还记得ServerBootstrap的内部类ServerBootstrapAcceptor吗？         //该类的channelRead方法将被回调，该方法的这行代码childGroup.register(child).addListener()，就会把接收到的         //客户端channel以轮询的方式注册到workgroup的单线程执行器中         pipeline.fireChannelRead(readBuf.get(i));     }     //清除集合     readBuf.clear();     if (exception != null) {         throw new RuntimeException(exception);     } }`

在上面的代码块中可以看到，触发连接事件之后，可能会有很多客户端连接到来，所以服务端其实是在一个 do ··· while 循环中一直接收到来的客户端连接，直到没有连接了，才打破循环，执行后面的代码。这个逻辑本身也没什么大问题，如果用 NIO 构建一个服务端，让这个服务端接收客户端连接的时候，完全可以使用上面代码块中的逻辑。可是回到 Netty 中，就没那么简单了。请大家想一想，在 Netty 中，IO 事件的处理其实是由单线程执行器来执行的，如果单线程执行器只负责处理 IO 事件，那么就可以在一个循环中无限接收客户端连接。比如在某个时刻，服务端接收到了几百个客户端连接，那单线程执行器就可以在循环中一直接收这几百个客户端连接，直到没有连接到来为止。可现在单线程执行器并不是只负责处理 IO 事件，在处理 IO 事件的时候还要负责处理用户提交的各种任务。如果把时间全用在接受客户端连接上，那用户提交的任务显然就得不到执行了。所以，在 Netty 中对 NioServerSocketChannel 做了一个限制，那就是每次接收到 OP_ACCEPT 事件之后，在 read 方法中接收客户端连接时，限制每次最多只能接收 16 个客户端连接，然后就不能再接收了。之后单线程执行器就会跳出处理 IO 事件的方法，如果这时候单线程执行器的任务队列中有用户提交的任务，那么单线程执行器就会执行这些任务。这就是 Netty 对 read 方法进行的重构。说到底，其实就是增加了一个接收客户端连接的限制，其他的也没什么重要的改进了，当然，也确实增添了一些方法的回调，但这个非常简单，我会展示在下面的代码中。

请看下面重构过后的 read 方法。

java

复制代码

`/** * @Author: PP-jessica * @Description:重构之后的，服务端接收客户端连接的方法，也是最终版本的方法 */ @Override public void read() {     //该方法要在netty的线程执行器中执行     assert eventLoop().inEventLoop(Thread.currentThread());     final ChannelConfig config = config();     //得到ChannelPipeline     final ChannelPipeline pipeline = pipeline();     //得到动态内存分配的处理器，注意，这里得到的并不是一个分配器，而是一个handle     //这个动态内存分配器是新引入的类型，这个类是客户端channel和服务端channel共用的     //在这个类中定义了最大连接次数，也就是16，具体的代码我就不在文章中展示了     //代码的注释中写得非常清楚，我不愿意展示是因为不想在文中贴太多代码，只把核心改进讲解了，剩下的就可以去代码中学习了     final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();     //把配置类传进去，这里方法的具体作用其实就是在再次开始接收客户端连接前，重置一下接收次数，就是把上一次接收连接的次数清零     allocHandle.reset(config);     boolean closed = false;     Throwable exception = null;     try {         try {             do {                 //开始接收客户端连接，这里只要接收到客户端连接了，就会返回1。如果返回的值为0，说明没有客户端连接可被接收                 int localRead = doReadMessages(readBuf);                 //这里就意味着已经没有客户端连接可被接收了，直接退出这个循环即可                 if (localRead == 0) {                     break;                 }                 if (localRead < 0) {                     closed = true;                     break;                 }                 //在这里累加接收到的客户端的连接次数                 allocHandle.incMessagesRead(localRead);                 //这里会判断接收连接的总次数是否达到16次了，如果达到了就要退出循环了                 //其实不管是接收客户端连接还是读取消息，都是限制16次，这个限制的作用其实也很简单，就是把执行机会让出去                 //不让这一个单线程执行器只干着一件事，还有其他的用户设定的任务也等待这个单线程执行器去执行呢。而且，这个单线程                 //执行期管理着多个客户端连接，不能总读取这一个客户端连接的消息吧             } while (allocHandle.continueReading());        } catch (Throwable t) {            exception = t;        }        //这里可以获得接收到的客户端连接的总个数        int size = readBuf.size();        for (int i = 0; i < size; i ++) {            //走到这里肯定就是没有可以继续接受的数据了，所以要把这个属性置为false            readPending = false;            //回调ChannelRead方法，在这个回调方法中，会把接收到的客户端连接注册到工作线程的selector上            //可以看到ChannelRead方法是每接收一个客户端连接就要被回调一次            pipeline.fireChannelRead(readBuf.get(i));        }        //清除接收到的客户端连接        readBuf.clear();        //这里这个方法其实没什么用，在读取客户端发送的消息时，这个方法会记录本次读取到的总的字节数        //但是在接收客户端连接时，这个方法只是一个空实现        allocHandle.readComplete();        //回调ChannelReadComplete方法        //这个方法只会被回调一次，在所有连接接收完了之后被回调        pipeline.fireChannelReadComplete();        //下面都是关闭连接的一些相关操作，这里就先不详细展开了        if (exception != null) {            closed = closeOnReadError(exception            pipeline.fireExceptionCaught(exception);        }        if (closed) {            inputShutdown = true;            if (isOpen()) {                 close(voidPromise());             }         }     } finally {         if (!readPending && !config.isAutoRead()) {             removeReadOp();         }     } }`

可以看到，上面代码块的逻辑还是挺多的，但核心逻辑就像我刚才为大家分析的，在一个循环中接收客户端连接，最多接收 16 个或者没有客户端连接了，就会跳出循环了。当然，在上面的代码块中我为程序引入了一个新的类型，就是 RecvByteBufAllocator，其实它本身是个接口，有两个实现类，一个为 DefaultMaxMessagesRecvByteBufAllocator 类，另一个为 AdaptiveRecvByteBufAllocator 类。在服务端接收客户端连接的时候，使用到的就是 DefaultMaxMessagesRecvByteBufAllocator 类。而在客户端接收字节消息的时候，使用的就是 AdaptiveRecvByteBufAllocator 类。我在这里为大家简单介绍一下，这样一来，大家看源码的时候也知道去哪个类中学习。到此为止，NioServerSocketChannel 的 read 方法就重构完毕了。

**重构 NioSocketChannel 的 read 方法**

NioServerSocketChannel 的 read 方法重构完了，接下来就该分析分析，NioSocketChannel 的 read 方法该如何重构了。在分析之前，还是先来简单回顾一下，过去版本的代码中，NioSocketChannel 是怎么通过 read 方法来接收字节消息的。当然，首先肯定还是 NioSocketChannel 接收到了 OP_READ 事件，然后调用 Unsafe 对象的 read 方法，最终会执行到 Channel 的 read 方法。这个逻辑在上一小节已经梳理过了，所以就不再重复展示代码块了。下面，我直接为大家展示重构之前的 NioSocketChannel 的 read 方法。请看下面代码块。

java

复制代码

`@Override public final void read() {     //得到ChannelPipeline     final ChannelPipeline pipeline = pipeline();     //暂时用最原始简陋的方法处理     ByteBuffer byteBuf = ByteBuffer.allocate(1024);     try {         int bytes = doReadBytes(byteBuf);         //源码中并没有下面这个判断分支，这里这么写是为了再客户端channel关闭的时候，服务端可以不报错。后面我们会逐步完善。         if (bytes == -1) {             return;         }         //把数据向后面的handler传递做处理         pipeline.fireChannelRead(byteBuf);         //新增加一个方法，这个方法是为了配合心跳检测使用的，暂时写成这样，等后面重构read方法时候会再次讲到该方法         pipeline.fireChannelReadComplete();     } catch (Exception e) {         e.printStackTrace();     } } @Override protected int doReadBytes(ByteBuffer byteBuf) throws Exception {     int len = javaChannel().read(byteBuf);     if (len == -1) {         javaChannel().close();         return -1;     }     byte[] buffer = new byte[len];     byteBuf.flip();     byteBuf.get(buffer);     //返回读取到的字节长度     return len; }`

可以看到，上面代码块中的 read 方法十分简陋，看着就很粗糙，所以，先让我们看看上面的代码块中都存在什么问题吧。我一眼就看见了 ByteBuffer，惊讶地发现我手写的 Netty 进行到这个程度了，竟然还是 ByteBuffer 来接收字节消息，我明明都已经为大家讲过 ByteBuf 了呀。所以，首先要重构就是用 ByteBuf 来存放接收的字节消息，让 ByteBuffer 隐藏在 Netty 的底层吧。第二个问题其实也很明显了，那就是 NioServerChannel 接收字节的情况。请大家跟着我的思路想一想，如果 NioServerChannel 接收到了字节消息，比如说就是一个客户端给服务端发送消息了，一次性发送过来 4 KB 的字节消息，现在服务端管理的 NioServerChannel 要接收这些消息了。在上面的代码块中，直接分配了一个容量为 1 KB 的 ByteBuffer' 对象来接收字节消息，并且只接收了一次消息，然后就执行下面的代码了。但现在的情况是发送过来的字节消息是 4 KB 呀，显然只接收一次消息是不可能把这次发送过来的所有消息全部接收完的，因为分配出来的 ByteBuffer 只有 1 KB，根本存放不下 4 KB 的字节消息。这该怎么办呢？程序编写有问题，那是因为我偷懒了，觉得一次最多只接收到几十个字节，不会再多了，所以按照上面那种实现逻辑，在接收到消息并不多的情况下，程序仍然是可以运行的，但这并不代表程序没有问题。相反，程序存在着十分严重的缺陷。只要接收到的消息特别多的时候，问题就会立刻暴露出来！

很好，既然现在我们已经知道了处罚问题的原因，那就针对这个原因着手解决即可。接收消息特别多的时候程序就会出问题，那么我们就按照常规思路设想一下，在接收到消息的时候，其实完全可以循环接收消息，而不是只接收一次。直到没有消息了，就跳出循环，继续执行后面的代码。而用来接收字节消息的 ByteBuf 的容量其实也很容易设计，按照常规思路，肯定是在循环中第一次接收字节消息的时候，先分配一个默认大小的 ByteBuf 对象，比如就分配容量为 1 KB 大小的 ByteBuf，然后根据本次循环接收到的字节消息，动态调整下一次要分配的 ByteBuf 对象的容量大小。比如说，第一次分配了 1 KB 容量的 ByteBuf 对象，接收到的字节消息把这个 ByteBuf 填满了，那么就默认还有很多字节消息要接收，1 KB 的容量显然不够，所以在下一次循环的时候，就分配 2 KB 容量的 ByteBuf，但是这一次只接收了 512 个字节之后就没有消息了，那么本次循环读取数据就结束了。但是，这一次分配的 2 KB 容量的 ByteBuf 最后只存放了 512 个字节，这也许就意味着下一次来自对方的消息也许不会有那么多了，所以，下一次接收消息的时候，会分配一个容量较少的 ByteBuf 对象来接收字节消息。总之，NioServerChannel 接收字节消息时肯定要使用 ByteBuf 对象，而每一次分配的这个 ByteBuf 对象的容量大小并不是一成不变的，会根据每一次接收到的字节消息的总大小做判断，然后决定下一次分配的 ByteBuf 的容量比上次是扩容了，还是缩容了。

我想这个逻辑应该也很简单吧，如果这个逻辑大家都理解了，那么我们就要再次探讨一个老问题。就是 NioServerChannel 通过 read 方法接收字节消息的时候，需不需要施加什么限制？刚才我们已经分析了，NioServerChannel 肯定会在一个循环中接收字节消息，如果字节消息非常多，就在循环中分好几次接收完，并且还会根据接收到的字节消息，动态调整下一次要分配的 ByteBuf 对象的容量大小。当然，如果第一次循环就把消息接收完了，那么第二次循环判断接收到的字节为 0 后，就会跳出循环了。这个逻辑看似很完美，但是请允许我提醒一点，那就是 NioServerChannel 接收字节消息的 read 方法实际上也是由单线程执行器来执行的，而单线程执行器不仅要处理 IO 事件，还要及时执行处理用户提交的各种任务。并且，还有最重要的一点是不能忽略的，那就是单线程执行器管理着多个 NioServerChannel，原因很简单，一个单线程执行器持有者一个 Selector，一个 Selector 会被多个 SocketChannel 注册，这也就意味着一个 Selector 要管理多个 SocketChannel，也就意味着一个单线程执行器要处理多个 SocketChannel 的 IO 事件。如果单线程执行器管理的多个 SocketChannel 在某个时刻都触发了 OP_READ 事件，并且在这些 SocketChannel 中有一个 Channel 要接收的字节消息非常多，别说循环 10 次，就是循环 50 次也很难把这些消息都接收完，难道就要让这个循环一直持续下去吗？换句话说，难道就要让单线程执行器一直在这个循环中处理这一个 SocketChannel 的 OP_READ 事件吗？显然是不能的，否则其它 SocketChannel 的 OP_READ 事件以及用户提交的任务都没办法及时处理了。所以，NioSocketChannel 的 read 方法中的循环也应该施加一个限制，这个限制就是循环的次数最大为 16 次，超过 16 次就强制终止循环了，即便还有未读取完的数据，那也留到下次再读取。

到此为止，我就为大家分析完了 NioSocketChannel 的 read 方法的重构思路了。其实总结下来也就三点，一是将 ByteBuffer 对象替换成 ByteBuf 对象，使用 ByteBuf 对象来存放字节消息；二是根据每次循环中接收到的字节消息的大小，动态调整下一次要分配的 ByteBuf 对象的容量大小；三是循环接收字节消息的时候，最多只允许循环 16 次。好了，文字分析已经写得够多了，接下来，就让我们从代码层面看看重构后的逻辑吧。请看下面代码块。

java

复制代码

`/** * @Author: PP-jessica * @Description:重构之后的读取消息的方法 */ @Override public final void read() {     //得到配置类     final ChannelConfig config = config();     if (shouldBreakReadReady(config)) {         clearReadPending();         return;     }     final ChannelPipeline pipeline = pipeline();     //得到内存分配器，这个是真正的内存分配器     final ByteBufAllocator allocator = config.getAllocator();     //得到动态内存分配器的处理器，这个处理器要配合内存分配器来使用，这个就是客户端channel要使用的AdaptiveRecvByteBufAllocator类     //AdaptiveRecvByteBufAllocator类也是RecvByteBufAllocator接口的实现类     final RecvByteBufAllocator.Handle allocHandle = recvBufAllocHandle();     //和之前学习的服务端接收客户端连接时一样，下面这个方法同样是用来在本次接收数据之前，重置一下状态。就是把上一次接收到的数据清零     allocHandle.reset(config);     ByteBuf byteBuf = null;     boolean close = false;     try {         do {             //分配一块内存，注意，如果是第一次来读取字节消息，但是并没有上一次接收到的字节数量做参考，我们怎么知道第一次要分配             //多大的ByteBuf呢？所以这里有一个默认值，1024，第一次会分配1024个字节大小，用于接收数据             byteBuf = allocHandle.allocate(allocator);             //doReadBytes(byteBuf)方法返回的是本次接收到的字节数量。然后把该值记录下来。             //在lastBytesRead方法中也会判断，如果这一次循环中接收到的字节把ByteBuf填满了             //那么下一次循环会分配一个更大的ByteBuf对象，也就是扩容了             allocHandle.lastBytesRead(doReadBytes(byteBuf));             //这里是得到本次读取到的数据，如果读取到的数据等于0，说明到此为止，客户端channel中的数据已经全部读取完了             //可以直接退出循环了             if (allocHandle.lastBytesRead() <= 0) {                 //退出循环之前，释放该ByteBuf                 byteBuf.release();                 //清除引用                 byteBuf = null;                 //如果本次接收到的结果小于0，就意味着客户端要关闭了，当接收为-1时，代表客户端channel要关闭                 close = allocHandle.lastBytesRead() < 0;                 if (close) {                    //把该属性置为false                     readPending = false;                 }                 break;             }             //累加读取消息的次数，到达16次就不能再继续读取了。理由之前我们已经分析过了             //因为单线程执行器掌管着多个channel，不能把执行事件都给了一个channel，也要给其他channel机会，更要注意             //还有很多用户提交的异步任务等待单线程执行器去执行。这个详细的流程可以在我们重构了NioEventLoop的run方法时看到             allocHandle.incMessagesRead(1);             readPending = false;             //回调handler中的channelread方法，该方法也是每读取一次数据就回调一次             pipeline.fireChannelRead(byteBuf);             byteBuf = null;             //判断是否要结束循环了         } while (allocHandle.continueReading());         //进行到这里就算是本次读取数据已经完结了，要根据本次读到的所有数据判断下次是否应该扩容或者缩容         //也就是下次分配ByteBuf时，是否应该分配一个持有内存更大或者更小的ByteBuf         allocHandle.readComplete();         //回调channelReadComplete方法，可以看到，该方法在读取完数据之后会被回调，并且只被回调一次         pipeline.fireChannelReadComplete();         //下面都是一些和关闭连接相关的操作，暂时不详细展开了         if (close) {             closeOnRead(pipeline);         }     } catch (Throwable t) {         handleReadException(pipeline, byteBuf, t, close, allocHandle);     } finally {         if (!readPending && !config.isAutoRead()) {             removeReadOp();         }     } } /**  * @Author: PP-jessica  * @Description:重构之后的读取消息的终极方法  */ @Override protected int doReadBytes(ByteBuf byteBuf) throws Exception {     //得到动态内存分配器的处理器     final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle();     //这里先得到ByteBuf的可写字节数，然后将这个可写字节数赋值给处理器中的attemptedBytesRead属性     //为什么要这么做？因为最后读取到的字节数和这个可以入的字节数相等了，说明这次读取数据已经满了，ByteBuf已经装不下数据了     //但是这并不意味着channel中就没有可读取的数据了，这只能说明这个ByteBuf没办法再写入数据了     //如果是另一种结果，就是最后读取到的字节数小于这个可写入的字节数，说明channel中的数据已经全部读取完了     //总之，这个属性被赋值了，就可以很容易判断出读取了之后，客户端channel中是否还有数据可以被读取     //这个byteBuf.writableBytes()的可写入字节数每次都是会变化的，这个要弄清楚     allocHandle.attemptedBytesRead(byteBuf.writableBytes());     //在这里把客户端channel和可写的字节数传进方法内，数据是要从客户端channel中写入到ByteBuf中的     //这里就会把数据从channel写到ByteBuf中了     return byteBuf.writeBytes(javaChannel(), allocHandle.attemptedBytesRead()); }`

上面代码块中的注释十分详细，把每行代码的作用都解释清楚了。当然，动态调整 ByteBuf 扩容缩容的逻辑并没在上面的代码块中展示，这里说的逻辑指的是如果 ByteBuf 要扩容或者缩容了，要按照多大的规格来扩容和缩容，这部分逻辑就在 AdaptiveRecvByteBufAllocator 类中，这个类我已经在文中提过多次了，但始终没有引入进来，给大家详细讲解一下，展示一下代码。坦诚地说，这个类，包括服务端 Channel 用到的那个 DefaultMaxMessagesRecvByteBufAllocator 类确实没什么可讲的，我不把它们引入文章有两点原因，一是要引入的话会搬运更多代码块，我搬运过来倒是可以，但是大家看了之后觉得特别简单，会觉得我有凑字数，混篇幅的嫌疑。我本身也不想搬运太多代码，如果我能把知识点讲解清楚，大家自己去源码中学习，这样岂不是效果更佳？二是这两个类也确实简单，难易程度就好比带女朋友打过战斗砖块剧场的第一关，所以就留给大家去看吧。

当然，我肯定也不会一点都不讲，现在我就简单为大家讲解一下按照什么规格动态调整 ByteBuf 的容量大小。Netty 中给 ByteBuf 对象的容量最大值做了一个限制，就是 65536 字节，也就是说就算动态调整无限扩容，最后容量也不能超过 65536 字节。ByteBuf 的初始化容量就是 1024 字节，最小容量为 64 字节，所谓最小容量，就是 ByteBuf 再怎么缩容，容量也不会小于 64 字节。并且在 Netty 中已经给 ByteBuf 定义好了扩容缩容的数值了，在 AdaptiveRecvByteBufAllocator 类中有一个数组，数组中由小到大存放的就是 ByteBuf 的各个容量，从 64 开始递增，在递增到 512 之前，一直是按照 16 字节来递增的，而递增到 512 之后，就是按照一倍的大小递增的。就像下面代码块中展示的这样。

java

复制代码

`//这个数组就是扩缩容的容量表，用数组记录，每一个下标都对应着一个容量大小 private static final int[] SIZE_TABLE; //数组的内容就是下面这样 64 80 96 ······ 512 1024 2048 ······`

ByteBuf 需要扩容了，就从 SIZE_TABLE 容量表中找一个合适的大小来扩容，需要缩容了，也从容量表中找一个合适的大小进行缩容，至于怎么寻找合适的数值，这个逻辑也在 AdaptiveRecvByteBufAllocator 类中实现了。就让大家自己去看吧。我上面讲解的关于动态调整 ByteBuf 容量的逻辑几乎都在 AdaptiveRecvByteBufAllocator 类中得到了实现，大家只需看这个类即可。

这一章的内容到此也就结束了，我们下一章见。

**总结**

这一章的内容也是非常少，就是客户端和服务端接收数据的 read 方法的重构，重构的思路也很简单。当然，思路简单不一定代码实现就简单，文章中讲得少不一定代码就少。也没什么可多说的，大家尽快去我提供的源码吧。