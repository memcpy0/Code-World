大家好，上一章我已经把自己的内存分配系统的进度推进到从 PoolChunk 对象中分配 8 KB 内存了。经过上一章的讲解，我相信大家应该也能理解为什么要这么做。因为对内存分配的规格有硬性规定，分配小于 8 KB 的内存和分配 8 KB 或者大于 8 KB 内存的流程是不同的。所以，现在就让我们直奔主题吧，先来看看怎么从一个 PoolChunk 对象中管理的 16 MB 堆外内存中分配 8 KB 内存。

**分割 PoolChunk 的方法**

其实讲解到第四章，知识已经讲了很多了，如果我们不考虑小于 8 KB 的内存的分配，或者直接说不考虑小额内存的分配。实际上，在我们剖析了 PoolChunk 如何分配 8 KB 的内存后，整个内存分配模块的知识差不多究竟完了，最多再加一个内存池的实现，但这个实现非常简单，比对象池简单多了。但知识就摆在这里，不得不讲，所以在讲完 PoolChunk 对象分配 8 KB 内存的方式后，还要继续剖析分配出来的 8 KB 如何进一步为小额内存的申请和分配提供服务。虽然分配的原理都是一致的，但是我想学到这里大家或多或少都都有倦怠了。所以，我们不妨就从一个简单的小例子入手，然后渐渐扩展到真正的内存分配系统中。

当然，即便是一个简单的小例子，看懂它也是有前提的。我不知道大家对 PooledDirectByteBuf 和其父类 PooledByteBuf 的内部结构还有没有印象，确切地说，我希望大家还没忘记分配出来的堆外内存是怎么包装给 PooledDirectByteBuf 对象的。这些知识是第 22 章的内容了。在第 22 章中，我讲解了堆外内存分配出来后被包装的方式，其实就是把 PoolChunk 对象持有的 memory 成员变量，赋值给 PooledByteBuf 中的成员变量 memory。而这个 memory 其实就是申请到的了堆外内存，其实也就是一个 ByteBuffer，而申请到的堆外内存被 ByteBuffer 管理。具体的管理方式是通过 ByteBuffer 中的 Unsafe 对象来操作分配到的堆外内存的地址，这个地址也在 ByteBuffer 中，是它的一个成员变量。但是光有 memory 这个成员变量是不够的，如果 PooledByteBuf 对象只是得到了这个 memory，其实就是内部持有了这个 ByteBuffer，仅仅是持有这个 ByteBuffer 有什么用呢？仍然不知道从这个 ByteBuffer 包装的 16 MB 堆外内存中分配出来的内存地址是多少，只有知道这个地址，才能让 Unsafe 对象直接操纵这个地址，然后就可以以这个地址为起点开始向空闲内存写入字节了。所以，PooledByteBuf 对象不仅要得到 ByteBufer，还要知道从 ByteBuffer 包装的堆外内存中分配出来的内存地址，但实际上堆外内存分配出来的只是一个偏移量，代表着分配的内存地址相对于 16 MB 堆外内存的起始地址的偏移量。有了这个偏移量，就可以通过 16 MB 堆外内存的起始地址加上内存偏移量，得到 Unsafe 对象最终可以直接操纵的地址，然后就可以写入数据了。这里我只是简单带着大家回顾了一下，如果详细展开讲解，又要写很多内容。如果大家忘记这一块的知识了，就翻回到前面复习一下吧。在这里，我只把 PooledByteBuf 类的代码搬运过来供大家复习。请看下面代码块。

java

复制代码

`public abstract class PooledByteBuf<T> {     //这个属性就是表明了这个PooledByteBuf对象使用的     //堆外内存是从哪个PoolChunk中分配来的     protected PoolChunk<T> chunk;     //把PoolChunk管理的ByteBuffer的引用也赋值给这里     //这样一来，该类也可以直接使用ByteBuffer的方法来直接操作堆外内存了     protected T memory;     //每创建一个PooledByteBuf对象，就要从PoolChunk持有的堆外内存中分配一块内存     //但是并不直接返回分配到的内存起始地址，而是返回分配到的内存地址距16MB堆外内存起始地址的偏移量     protected int offset;     //PooledByteBuf对象分配到的内存大小     protected int length;     //给成员变量赋值的方法，该方法会在PoolChunk类中被调用     void init(PoolChunk<T> chunk,int offset, int length) {         init0(chunk, offset, length);     }     private void init0(PoolChunk<T> chunk,int offset, int length) {         assert chunk != null;         this.chunk = chunk;         memory = chunk.memory;         this.offset = offset;         this.length = length;     }     //写入字节的方法，这里的index参数，其实很好理解。因为在用户看来他得到的是一个ByteBuf     //对象，他操作的就是这个对象中的字节容器，这个字节容器对用户来说就是从0到其容量这么大     //所以用户指定了index为几，就希望把字节消息从字节容器的第几位开始写入     public ByteBuf setByte(int index, int value) {         memory.put(idx(index), (byte) value);     }     protected final int idx(int index) {         return offset + index;     } }`

如果上面把内存交给 PooledByteBuf 对象包装的逻辑，大家都理解了，那么请跟着我一起看下面这个例子。假如现在 PoolChunk 包装的堆外内存并不是 16 MB，而是 16 个字节。这 16 个字节操作系统已经为我申请成功了，现在交给了 PooLChunk 对象管理，就像下面这幅简图展示的这样。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ea73bde0047e4df38ae609526ea8af39~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1686&h=458&s=19941&e=jpg&b=fefefe)

如果现在程序内部要申请内存，就从这 16 个字节中申请内存，并且，我们再给程序设定一个理想状态。程序内部每次申请内存都是按照 4 个字节来申请的。这样一来，这 16 个字节其实就可以均分为 4 个 4 字节的内存快。假如现在要进行程序内部的内存分配，要从这 16 个字节中申请 4 个字节，当这 16 个字节都是空闲的时候，肯定会从头开始分配，把 0 - 3 字节分配出去。当然，按照我们上面讲解的分配流程，PooledByteBuf 对象会获得 PoolChunk 对象中 memory 成员变量的引用，也就是获得了管理堆外内存的 ByteBuffer 对象的引用，也就间接获得了分配的 16 个字节的堆外内存的起始地址，也就是上面简图中字节数组中的 0 号索引地址。而现在是把上面字节数组中的 0 - 3 字节分配给 PooledByteBuf 对象，因为是从 0 号索引位置开始分配，所以分配的内存地址相对于 16 字节的堆外内存的起始地址的偏移量是 0。所以 PooledByteBuf 对象中的 offset 成员变量就会被赋值为 0。这样一来，堆外内存的起始地址 0 加上内存偏移量 0 得到的仍然是 0，所以在真正操纵内存地址，写入字节消息的时候，Unsafe 对象就会以 0 为起始地址开始写入字节消息。当然，分配的内存长度肯定是确定了，就是用户申请的内存大小，PooledByteBuf 对象中的 length 成员变量就会被用户申请的内存大小赋值。由此也可以看出，虽然分配内存的时候并没有从堆外内存中对要分配多少内存做了限制，但是这个限制一开始就明确了，是由用户定义的。所以写入字节消息的时候，写到字节数组的 3 号索引位置时，就不能再往后写了，因为 PooledByteBuf 的容量就是这么大，如果想继续写入字节消息，就需要给 PooledByteBuf 对象动态扩容了。所谓动态扩容，其实就是从 16 个字节中分配一块更大的内存而已。

上面的例子讲完了，我相信不会有人看不明白，因为例子十分简单。如果是这样，现在我想提出一个新的要求。那就是我们的程序目前不是只以 4 个字节的方式来申请内存了，除了申请 4 个字节大小的内存，还会时不时的申请 8 个字节大小的内存。这个时候该怎么规划已经得到的这 16 个字节的堆外内存呢？如果不做规划，就正常分配的话，其实也没什么问题，比如说，用户第一次申请的是 8 个字节大小的内存，那就把字节数组的 0 - 7 个字节分配出去；紧接着第二次内存申请来了，这次申请的是 4 个字节，既然 0 - 7 的字节已经被使用了，那么就把 8 - 11 的字节分配出去吧，分配出来的内存地址相对于堆外内存的起始地址的偏移量是 8，所以正好就可以把 8 赋值给 PooledByteBuf 对象的 offset 成员变量。如果第三次内存申请来了，申请的是 8 个字节，可这时候内存显然不够了，只剩下 4 个字节了呀。所以就会创建一个新的 PoolChunk 对象，同时申请 16 个字节的堆外内存交给 ByteBuffer 对象管理，然后把 ByteBuffer 对象的引用交给 PoolChunk 对象。然后再从新的 PoolChunk 对象中分配 8 个字节交给用户。这个逻辑也很简单，让人觉得一切都那么顺理成章，但是，我不得不点破最关键的一个问题，那就是怎么保证分配出去的内存不会再被分配出去呢？这个问题从第 21 章就提出来了，当时我没能给大家解决，现在我们又一次遇到了这个问题。而且到了不得不解决的时候了，因为只要解决完了这个问题，我的内存分配系统差不多就搭建完成了。

其实除了上面那个问题，内存分配系统中还存在着另一个问题，那就是精准定位可分配的内存时效率不够高。还是以刚才那个例子来解释，当用户第一次从 16 个字节的堆外内存中申请 8 个字节时，正常逻辑肯定是从字节数组的头部开始查看，如果内存可以分配，就直接把内存分配出去。紧接着第二次申请 4 个字节时，还是会从头部查看，发现不能分配，于是就越过这 8 个字节，分配这 8 个字节之后的内存。那么，假如申请到的堆外内存有 512 个字节呢？按照 8 个字节和 4 个字节分配给用户的时候，已经分配了很多内存了，再有用户来申请内存的时候，岂不是要从数组的头部依次向后排查，直到遇见可以分配的内存，然后把内存分配出去。当然，这个逻辑是我们抽象出来的，而且我们还不知道怎么保证分配出去的内存不会再被分配出去。但现在我们就当这个问题解决了，然后按照刚刚分析的思路，来设计代码，编写代码。编写完之后，可能会觉得精准定位可分配内存的效率实在是不高。那该怎么办呢？如果让我换个思路来设计内存分配，其实我更希望能把相同规格的内存集中分配。这句话非常模糊，大家可能根本不明白是什么意思，因为我一时之间也找不到特别形象的语句来形容自己的点子，所以，就让我仍然用图文来给大家讲解吧。

请大家跟着我思考这样一个场景，现在申请到的堆外内存还是 16 个字节，也就是说 PoolChunk 对象包装的堆外内存是 16 字节。这时候，仍然是按照 8 字节和 4 字节的方式来向 PoolChunk 申请堆外内存。如果这次我变换一个方式，比如说我把 8 字节的内存归类到一起，4 字节的内存归类到一起。就像下面这幅简图展示得这样。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9104e764ecbf4f6b826e53a107d29b1a~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=2502&h=947&s=112350&e=jpg&b=fefefe)

请大家结合着上面的图片思考一下，申请到的堆外内存的总字节数为 16，这 16 个字节可以当作一个大的字节数组直接来分配字节，也就是上图第 0 层展示字节数组那样；当然，也可以分割成两个 8 字节的数组，如果程序中有地方要申请内存了，并且申请的是 8 字节的，那么直接就向这两个 8 字节的数组来申请即可，哪一个是空闲的，直接就分配出去就行了。就是上图第 1 层数组展示的那样。除了以 8 字节的大小分割，这 16 个字节还可以按照 4 个字节的大小分割成 4 个小字节数组，就是上图第 2 层展示的那样。如果用户要申请 4 个字节的内存，那么直接找第 2 层的字节数组来申请即可。当然，虽然在上面的简图中看起来 16 个字节的数组被分割开了，但是我希望大家不要理解错了，之所以分割开是因为是因为看起来更形象，实际上申请到的 16 个字节的堆外内存仍然是一个整体，并没有真的被分割开；并且最重要的是，虽然上图展示了 3 层字节数组，每一层的字节总数都为 16，但是并不意味着堆外内存的总数变成了 16 * 3，堆外内存仍然为最初申请到的 16 个字节，只不过按照分配规格的不同，我把这 16 个字节内存结构成了 3 种样式。第一种就是第 0 层展示的字节数组，就是仍然是 16 个字节，不管是分配 8 字节还是 4 字节的，都来这个字节数组分配即可。如果第一次申请了 8 字节，第二次申请 4 字节，那么第二次申请的时候还要看看前面分配了几个字节，然后才能得到一个内存地址偏移量。第二种就是第 1 层展示的那样，每次按照 8 个字节分配，第一次申请 8 字节，直接把第 0 块字节数组返回即可，偏移量正好是 0，如果是第二次申请，直接把第 1 块字节数组返回即可，因为第二层是按照 8 字节切割的，所以偏移量直接就是 8，直接就可以返回内存偏移量，当然，如果第 1 层有多个 8 字节的自己数组，那么字节数组在第 1 层是第几块，内存偏移量就是这个块数乘以 8。如果是第 N 块，那么分配出去的内存地址的内存偏移量相对于堆外内存的起始地址就是 N * 8。第三种情况就是上面简图中展示的第 2 层字节数组，可以看到，在这一层中，16 个字节的数组被分割成了 4 个 4 字节的小数组。如果申请的是 4 字节的内存，那么直接就来第 2 层申请即可，根据分配到的小字节数组是该层的第几块，然后乘以 4，就能得到内存偏移量，这个内存偏移量，最终就能赋值给 PooledByteBuf 对象中的 offset 成员变量。这个逻辑和第 1 层的分配逻辑是一样的，所以就不再重复了。

好了，这个逻辑讲完之后，让我来为大家梳理一下，现在内存分配的流程，也就是内存分配的链路终于到达 PoolChunk 类中的 allocate 方法后，具体要怎么进行内存分配了。在 PoolChunk 的 allocate 方法中，首先会根据用户申请的内存，当然这个内存是经过程序规整的，也就是使用经过规整后的内存，计算出要分配的内存大小位于上面简图的第几层，如果用户正好申请 16 字节，那要申请的内存就位于上面简图的第 0 层，要从第 0 层分配；如果经过规整后的内存是 8 字节，那就位于上面简图的第 1 层，要从第 1 层开始分配内存；如果经过规整的内存是 4 字节，那么就位于上图的第 2 层，要从第 2 层分配内存。计算出具体要从哪一层开始分配后，就要从这一层的头部开始查看，如果发现该层的第 0 块数组分配出去了，就去查看第 1 块数组是否分配出去了，如果没分配出去，就把这一块字节数组分配出去，也就是计算这一块数组在这一层中的内存偏移量，是第几块数组，就用那个数值乘以对应的字节数。如果是第 2 层的第 3 块数组，那么内存偏移量就是 3 * 4，也就是分配出去的这块内存相对于堆外内存的起始地址偏移了 12 个字节长度。讲解到这个程度，我相信大家应该也就终于清楚了 Netty 中的内粗分配是怎么回事了吧？那么这种分配方式给程序带来了什么好处呢？好处已经显而易见了，计算内存偏移量更快了，或者说更方便了。因为每一层对应的字节大小是确定的，只要知道分配的是该层的第几块字节数组，就可以立刻计算出字节偏移量。

当然，这个流程中也存在着很多问题，或者说有些问题并没有解决，导致这个流程无法顺利进行下去。接下来，就让我来为大家分析一下。

**引入二叉树实现内存的不可重复分配**

第一个问题肯定已经困扰大家很久了，那就是怎么知道内存已经分配出去了呢？也就是说，怎么保证已分配的内存不可重复分配？这个问题至今还没被解决，所以也就不急于这一时了，接下来，让我们看看第二个问题。这个问题其实也很严重。请大家结合上一小节的那幅简图想一想，如果用户第一次申请的是 4 字节的内存，在 PoolChunk 的 allocate 方法中，就会计算出这 4 字节内存要从第 2 层分配，那么这个计算过程是什么样呢？换句话说，怎么计算出用户申请的内存要从第 2 层开始分配呢？说得再直接一点，怎么用代码把 16 个字节划分为 3 层呢？至于第三个问题就更严重了，如果刚才用户申请的 4 个字节已经分配出去了，就是第 2 层的第 0 块数组已经被使用了，现在用户又来申请内存，但这次申请的是 8 字节的，在 PoolChunk 的 allocate 方法中经过计算后，显然会从第 1 层分配这 8 个字节，并且会发现第 1 层的第 0 块数组好像还未被使用，于是就分配出去了。但情况真的是这样吗？这么做显然是错误的，在一开始我就跟大家说过了，我们申请的堆外内存一直只是 16 字节，尽管图中划分成了 3 层，但并不意味着申请的堆外内存成了 16 * 3，简图中展示的只是 16 个字节的数组可以被 3 种方式分割而已。既然是这样，刚才用户已经申请了 16 个字节的前 4 个字节，第二次申请 8 个字节的时候，尽管简图中第 1 层第 0 块的 8 个字节的数组还未被分配出去，但实际上它内部的前 4 个字节已经被分配走了。这时候第 1 层的第 0 块数组显然已经不满足分配条件，只剩下 4 个字节了呀，但用户需要的是 8 个字节，怎么能够分配成功呢？所以这时候就要继续向第 1 层的第 1 块数组查看，如果它没被分配过，以及它的下一层的 4 个字节数组也没被分配过，那么就可以把第 1 层的第 1 块字节数组分配出去了。问题是，我们究竟该怎么设计，才能让第 1 层的 8 个字节，感知到下一层的 4 字节数组是否已经被分配了呢？说到底，这仍然是保证已分配的内存不可重复分配的问题。那这个问题究竟该怎么解决呢？

都讲到这个时候了，我不知道大家有没有注意到，其实上一小节的那个简图，就是一颗非常形象的二叉树。二叉树的顶点就是 16 个字节的堆外内存，顶点也就是二叉树的第 0 层，左子节点就是一块 8 字节的数组，右子节点也是一块 8 字节的数组，这就是二叉树的第 1 层，接着就是第 2 层。直接用二叉树来表示，就是下面这样。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2a3dd97ce6b9415ba8ebbc40f47beb9d~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1618&h=600&s=44919&e=jpg&b=ffffff)

上图就展示了一个总树深为 3 的二叉树。这个二叉树就恰好和上一小节的那幅简图对应上了。那么，这个二叉树怎么保证已分配的内存不可再分配呢？起始方法也很简单，就是给每一个字节数组施加一个标记。从上面的二叉树可以看出来，16 个堆外内存的字节中，可以分配的最小内存单位为 4 字节，这就像一个 PoolChunk 持有的 16 MB 堆外内存可以分配的最小内存单位为 8 KB 一样。现在，我要给这个深度为 3 的二叉树的每一层的每一个字节数组初始化一个标记。这个标记就是内存块所在的层数，比如说第 2 层的 4 块字节数组，只要它们还未被分配，那么每一块对应的初始值就是 2，第 1 层对应的是 1，第 0 层对应的是 0。就像下面这样。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/425134277f3d4aa18165150dfcb89afe~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1618&h=762&s=47553&e=jpg&b=ffffff)

如果现在用户想申请一块 4 字节的内存，经过计算之后，发现要从二叉树的第 2 层开始分配内存，然后把二叉树第 2 层的最左子节点分配出去了，这时候，就把这个节点对应的初始值 2 修改为 3。所谓 3，其实就是二叉树的深度。具体的变化请看下图。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e5eb2e45563a4c0c91123d32958af96d~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1618&h=762&s=47574&e=jpg&b=ffffff)

紧接着，用户要申请第二块内存了，不管申请的是多大的内存吧，现在还不忙着分配，现在我要解决一个问题，那就是让二叉树的第 1 层的每一个节点都感知到自己子节点的使用情况。比如说第 2 层的最左子节点已经分配出去了，最左子节点的兄弟节点没有分配。这就意味着二叉树第 1 层的最左子节点，也就是这个 8 字节已经不完整了，如果现在用户申请的是 8 字节大小的内存，经过计算发现要从二叉树的第 1 层开始分配。然后去查看二叉树第一层的最左子节点，正好是 8 字节大小。如果这个 8 字节大小的节点不知道自己的左子节点已经分配出去了，那么这时候就要把这个 8 字节分配给用户了。但显然不能这么做，对吧？因为这 8 字节中已经有 4 字节被用户使用了，重复分配不就造成数据错乱了吗？那怎么才能让二叉树第 1 层的左子节点，也就是这个 8 字节的节点知道自己已经不完整了呢？很简单，这个 8 字节的节点不也对应着一个初始值吗？修改这个初始值不就行了吗？当它的左子节点被分配了，左子节点的初始值就变成了 3，这时候，二叉树第 1 层的左子节点的两个字节点的初始值就分别是 3、2，分配出去的那个是 3，还未被分配的节点的初始值仍然是 2。现在我要比较二叉树第 2 层的最左子节点和它的兄弟节点对应的初始值的大小，然后把小的这个初始值赋值给它们父节点对应的初始值。简单来说，就是把二叉树第 1 层最左子节点的初始值，修改为其右字节点的初始值。其改动如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e54f63cdfd2141e0b19447ab4d4ae278~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1618&h=762&s=47675&e=jpg&b=ffffff)

如果已经修改完成了，那么当用户申请的字节大小为 8 字节，经过计算发现要从二叉树的第 1 层开始分配。然后去查看二叉树第 1 层的最左子节点，正好是 8 字节大小，本来可以把它分配给用户了，但是又对比了一下这个节点对应的初始值，发现已经不是自己所在的层数 1，而是 2 了。这就意味着这个节点已经被分配了一部分了，肯定不完整了，所以就不会从这个节点分配 8 字节，而是查看该节点的兄弟节点，也就是二叉树第 1 层的右子节点，发现该节点对应的初始值仍然是 1，所以直接把这个节点分配出去就行，而偏移量就是 1 * 8。讲到这里，大家是不是有一种恍然大悟的感觉，所谓的给分配的内存做标记，就是这种标记，来防止重复分配。当然，这时候也许用户会直接来申请 16 字节的内存，经过计算肯定是从二叉树的第 0 层开始分配对吧？但是我们都知道，这个 16 字节已经分配出去一部分了，显然已经不完整了，但是其对应的初始值仍然没变，这是不应该的。由此我们其实可以推导出来，当一个节点的内存被分配之后，不仅其对应的初始值要改变，还要递归修改其父节点的初始值，其父类被修改后的初始值就是其左右子节点中较小的那个初始值。就拿现在的这个例子讲解，当二叉树的第 2 层的最左子节点被分配出去后，其对应的初始值就被修改为 3，其父节点对应的初始值会被修改为 2，其父节点的父节点，也就是二叉树的顶点，其初始值会被修改为 1。因为二叉树顶点的两个字节点对应的初始值分别是 2、1，当然是用较小的那个初始值给二叉树顶点的初始值赋值。具体改动如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/153b921fd4264ca0b44572beba7c22e3~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1618&h=762&s=47603&e=jpg&b=ffffff)

如果这个逻辑理解了，那么请大家再跟我分析这样一个场景。当申请的这 16 字节已经分配了 4 字节后，也就是上面简图中展示的样子。现在用户又希望来申请 4 个字节，经过计算肯定还是会在二叉树的第 2 层来分配内存，然后从第 2 层的最左子节点开始向右查看适合分配的小字节数组。首先就会查看最左子节点，发现最左字节点对应的那个初始值已经是 3 了，本来它应该对应着 2，这就意味着这个节点代表的内存已经分配走了，所以会查看这个节点的兄弟节点，发现这个兄弟节点的初始值仍然是 2，说明这个兄弟节点代表的内存还没被分配，这时候就会把这个兄弟节点代表的内存分配给用户。并且分配走了之后，还会把这个兄弟节点对应的初始值修改为 3，同时还会递归修改其父节点的值，但这时候二叉树第 2 层的最左子节点和其兄弟节点对应的初始值都是 3 了，已经没有大小之分了，所以，就会把这两个节点的父节点的对应初始值也修改为 3，现在的二叉树就如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1c6c849e0c744eb494c341ccfa157aad~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=1618&h=762&s=47622&e=jpg&b=ffffff)

讲到这里，我想大家可能也发现这个规律了，那就是只要二叉树中一个节点对应的初始值是 3，就意味着这个节点代表的内存已经分配了，说得更精确一点，就是已经分配完了。所以，当整个 16 字节都分配完了，这颗二叉树的顶点对应的初始值就会被修改为 3。我想，这一点也是比较容易理解的了。到此为止，我就为大家把内存分配的整个流程都剖析完了，防止内存重复分配的方法也讲解完了，以及 PoolChunk 怎么被一颗二叉树划分等等(真正的 PoolChunk 对象管理的是 16 MB 的堆外内存，这 16 MB 内存其实会被划分为一颗深度为 12 的二叉树，二叉树的第 11 层，也就是最后一层，这一层的所有节点代表的都是 8 KB 大小的内存，光这一层 8 KB 大小的节点就有 2048 个，所以说 PoolChunk 分配内存的最小单位为 8 KB)，这些知识就是 Netyy 中内存分配涉及到的知识。我已经全部讲解完了，当然，这里的全部指的是 8 KB 内存分配的流程。小额内存分配的流程我还没有为大家讲解。讲了这么多，一直是文字描述，也确实该展示一下代码了，比如说分割 PoolChunk 的二叉树该怎么表示，怎么计算分配的内存要从二叉树的第几层开始分配，怎么修改二叉树的每个节点对应的初始值等等。这些都应该用代码展示出来，但是我就不为大家展示代码了，因为代码的内容实在是太多了，简单来说就是用数组来抽象出了二叉树，之后的都是一些数学计算，用到了大量的位运算。大家可以直接去我提供的代码中学习。而且，我不在文章中展示代码还另有原因，一是代码内容实在是太多，第二点就是我在提供的源码中添加的注释比我的整篇文章的内容都多，每一个类的每一个成员变量和每一行代码都有详细注释，我在这里给大家放一张代码截图，大家就明白了。请看下图。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/27052f4d80014198a266403636f9efee~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=2450&h=1840&s=850342&e=png&a=1&b=141313)

上图就是 PoolChunk 部分代码的展示，可以看到，我甚至把每一个位运算都给大家详细解剖了。所以，我何必在文章中重复注释中的逻辑呢？内存配的流程已经给大家剖析完整了，大家只需要带着对流程的思考去看源码，我相信一定可以看懂。所以，关于大额内存分配的知识讲解就到此为止了。

**内存池终于登场**

大额内存分配讲解完之后，关于 8 KB 和大于 8 KB 内存的分配流程，我相信大家一定已经非常熟悉了，接下来似乎就该为大家讲解小额内存分配的流程，也就是小于 8 KB 的内存是如何分配的。但是还不是很急，这篇文章还有七八千字才结束，如果用五千字讲解小额内存分配的流程，那么至少会给我留下两千多字，让我为大家简单讲讲内存回收的知识。起始经过上面内存分配的讲解，大家肯定都清楚了，在 Netty 中，所谓的内存分配不过是分配一个内存偏移量，然后把分配内存的那个二叉树节点对应的初始值修改一下，这样一来，就知道这块内存被分配了。那么，当这块内存被使用完了，该怎么归还呢？或者说归还的本质是什么？本质很简单，因为内存本身就是要重复利用的，所以所谓归还，就是把二叉树代表这块内存的节点的初始值重新恢复成所在层数。这样下次分配的时候发现初始值和原来一样，就可以分配了。就是这么简单，关于内存回收的注释我也写在源码中了，非常详细，大家可以简单看看。

内存回收的原理如此简单，那么，请大家想想内存回收的过程中会有什么问题吗？当你觉得没有任何问题的时候，就想想是不是会有并发问题，但你觉得你的程序没有并发问题的时候，就请想想你的程序性能怎么样。当然，在内存回收的过程中我们首先遇到的就是并发问题。这个很明显吧，内存分配的时候就是一个 PoolArena 对象被多个线程访问，所以需要同步锁来保证并发安全。内存回收的时候肯定也会出现多个线程同时回收内存的情况呀，针对于同一个 PoolArena 来说，肯定也会出现并发问题。举个组简单的例子，当释放内存的时候，PoolChunk 的内存利用率肯定会发生变化，也许就要在利用率不同的 PoolChunkList 链表之间移动，大家应该都还记得这个链表是干什么用的吧？多线程处理这些操作难免会发生并发问题，所以，使用同步锁肯定也是不可避免的。这一点毫无疑问。那么，接下来就有一个可以优化的地方了，既然使用同步锁是不可避免的，就注定多线程要串行执行任务了。那么，也就意味着性能的稍微下降，那么有没有什么方法能避免性能下降呢？很简单，那就是不使用同步锁呀，或者说不到万不得已不使用这个同步锁。那怎么就不使用这个同步锁了呢？

大家意识到了吗？似乎又回到了如何使用无锁化思想解决并发问题，这不就是对象池的老套路了吗？只不过这里池化的对象变成了内存。也就是说，当一个线程创建了一个 ByteBuf 对象之后，也从 PoolChunk 中申请到了一块堆外内存交给这个 ByteBuf 对象使用。使用完了之后并不想归还给 PoolChunk，那就可以把这块内存放到线程私有的对象池中，说是对象池，其实就是针对内存创建出来了一个内存池。而这个内存池我在上一章就给大家展示过了，就是那个 PoolThreadCache 类。在上一章，这个类的结构还很简单，请看下面代码块。

java

复制代码

`final class PoolThreadCache {     //持有的使用次数最少的heapArena     PoolArena<byte[]> heapArena;     //持有的使用次数最少的directArena     PoolArena<ByteBuffer> directArena;     /**      * @Author: PP-jessica      * @Description:构造器方法      */     PoolThreadCache(PoolArena<byte[]> heapArena, PoolArena<ByteBuffer> directArena) {         if (heapArena != null) {             this.heapArena = heapArena;             //被持有的引用计数加1             heapArena.numThreadCaches.getAndIncrement();         }         if (directArena != null) {             this.directArena = directArena;             //被持有的引用计数加1             directArena.numThreadCaches.getAndIncrement();         }     } }`

在内存分配的流程讲完之后，这个类的内容将更加丰富，在这里我也就不再展示了，我为大家提供的源码中注释十分丰富。我只是简单讲解一下，这个放在内存池中的内存，是怎么被包装的。请大家想一想，内存分配最终分配出来的内存其实就是一个内存偏移量，这个应该已经是大家的共识了。这个内存偏移量肯定是从一个 PoolChunk 对象中分配出来的，对吧？一个 PoolChunk 对象被一颗二叉树结构，然后找到要分配的内存是二叉树的第几层，并且是相对于那一层偏移了几个小的字节数组，这样不就得到偏移量了吗？所以，我是不是可以这么说，只要得到了这个内存偏移量，并且得到了 PoolChunk 这个对象，就可以精确找到要具体操作的内存地址了？因此，在 Netty 中就是这么操作的，把要放进内存池的这块内存所属的 PoolChunk 对象的引用和这块内存在 PoolChunk 管理的 16 MB 堆外内存中的内存偏移量一起交给一个对象包装一下，然后把这个被包装后的对象放进线程私有的内存池中。这个包装内存偏移量和其所属的 PoolChunk 对象的引用的对象，就是非常著名的 Entry 对象。到此为止，关于内存池的知识也就简单介绍完了，在我看来，这些知识都是旧知识，相对于内存分配来说，都属于边角料知识了。如果还像之前那样分析迭代一大堆，显然有点浪费时间了。所以，大家就亲自到源码中结合注释学习吧。至于什么时候要把内存存放到内存池中，肯定是释放内存的时候呀，释放内存时会先试着把内存放到内存池中，如果内存池中放满了，不能再放了，才会把内存归还给 PoolChunk 对象，之后，这块堆外内存就可以被再次分配了。当然，我刚才提到了内存池放满之后就不能再存放内存了，这就意味着内存池也是有容量的。而内存池之所以有容量，就是因为不想让线程持有特别多的内存，导致程序中其他地方无法申请充足的内存了。并且内存池也会定期清理自己持有的内存，如果发现持有的内存超过某个阈值，说明有很多内存根本没被利用，一直在内存池中放着，这时候内存池就会自身清空一次，把内存归还给 PoolChunk 对象。这就像利用率太低的 PoolChunk 对象会被释放，都是一样的道理。

**小额内存的分配流程**

接下来，就该为大家讲讲小额内存的分配流程了。要想掌握小额内存的分配流程，首先就得知道什么才是小额内存。所谓的小额内存，就是 Tiny 和 Small 类型的内存，这两种内存的范围分别为 16 字节到 496 字节，512 字节到 4 KB。如果用户申请的是 6 KB，或者是 7 KB，那么就会直接分配给用户一块 8 KB 大小的内存。而分配 8 KB 或者是大于 8 KB 的内存，都属于大额内存的分配了。这个内存的分配过程已经讲解过了。那么，小额内存该怎么分配呢？在上一章我跟大家简单解释过一下，小额内存的分配，其实最终也是要从 PoolChunk 中分配的，这么说也许有歧义，我们可以这样认为，所谓的小额内存分配，其实最终使用的仍然是 PoolChunk 对象管理的那 16 MB 堆外内存。而小额内存分配的第一步，当然是先从 PoolChunk 中申请一块 8 KB 大小的内存，之后，小额内存的申请和分配就从这 8 KB 中进行。所以，现在问题的关键就来了，拿到这 8 KB 内存后，怎么分配给申请小额内存的用户呢？其实也没有特别好的分配方法，在 Netty 中，仍然是采用了解构的思想来分配小额内存的，就是在分配大额内存的时候，我们不是把一个 PoolChunk 对象管理的 16 MB 堆外内存用二叉树表示了吗？把 16 MB 的内存用不同的方式均等分割，因此才有了这颗 12 层深的二叉树。现在要从 8 KB 中分配小额内存了，采用的思路也差不多，就是把这 8 KB 堆外内存均等分割，一点点分配给申请小额内存的用户。

那么，这 8 KB 内存究竟该怎么分配呢？也是像 PoolChunk 那样用一颗二叉树解构吗？这里就有点不一样了，在 Netty 中，这个 8 KB 内存是这样分配的：如果用户申请的是 16 字节的小额内存，那就把一个 8 KB 用 16 字节均分，也就是说可以一共分为 512 个 16 字节，然后把其中一个 16 字节分配给用户。如果用户申请的是 4 KB，那就用 4 KB 把 8 KB 均分，然后分配一个 4 KB 出去。如果申请的是 496 个字节，那就用 496 字节把 8 KB 均分，然后分配 496 字节给用户。到这里大家应该也能看出来了，小额内存的分配比较直接，如果用户要申请一块小额内存，就要使用一个从 PoolChunk 中分配出来的 8 KB 内存，把这 8 KB 内存用要申请的小额内存等分了，然后分配一块小额内存出去。如果这个逻辑大家都理解了，那么接下来，我想给大家讲讲这个 8 KB 的内存是怎么被包装的。其实，每当为了分配小额内存而从 PoolChunk 中申请一个 8 KB 内存之后，这块 8 KB 的内存就会被一个对象包装起来。然后把这个对象放到一个数组中。包装这块 8 KB 内存的类就叫做 PoolSubpage，我们应学习了 ByteBuf 对象是怎么包装一块堆外内存的，这个 PoolSubpage 包装内存的原理其实和 ByteBuf 一样，也是要持有一个内存的偏移量。我在下面的代码块中展示了该类的一些成员变量，可以看到，该类的成员变量 runOffset，就是要被内存偏移量赋值。

java

复制代码

`/**  * @Author: PP-jessica  * @Description:这个类的对象就是用来包装8KB内存的  */ final class PoolSubpage<T> implements PoolSubpageMetric {     //该8KB内存是从哪个PoolChunk中分配的     final PoolChunk<T> chunk;     //分配的这块8KB内存在Chunk二叉树数组中的下标索引     private final int memoryMapIdx;     //这个PoolSubpage包装的8KB内存在Chunk中的内存偏移量的大小     private final int runOffset;     //这个PoolSubpage的大小，也就是8KB     private final int pageSize;     ······     //这个属性对应的其实就是用户申请分配的小额内存，经过规整之后的内存大小，因为PoolSubpage会被这个内存大小均分     //比如8KB的内存可以分成两个4KB，也可以分成512个16B，用户申请的是多少的内存，就按规整后的内存把PoolSubpage均分     //加入到PoolArena的时候，也是根据均分之后的elemSize大小，来寻找PoolSubpage数组中对应位置的双向链表加入     int elemSize;          //该PoolSubpage一共分割成了多少个elemSize     private int maxNumElems; }`

在上面这个代码块中，还有一个成员变量很关键，那就是 elemSize，这个成员变量就会被均分 8 KB 的小额内存赋值。也就是说，用户申请的是 16 字节，那这个 8 KB 就会被 16 字节均分，elemSize 就会被 16 赋值。表示这一个 PoolSubpage 包装的 8 KB 内存是被 16 字节均分的。为什么要在 PoolSubpage 类中定义 elemSize 成员变量呢？原因很简单，因为就算一块 8 KB 的内存被均分为 512 个 16 字节了，这 512 个 16 字节的内存也不是一次就用完的，用户如果多次申请 16 字节的内存，或者申请 14 字节然后被规整为 16 字节了，肯定就会在程序内部查看所有被 PoolSubpage 对象包装的 8 KB 内存是被多少的小额内存均分的，如果发现是被 16 字节均分的，那就会直接从这个对应的 PoolSubpage 对象中分配 16 字节内存交给用户。可是这样一来就又会带来一个问题，经过前面的分析，现在大家应该都意识到了，如果用户申请了很多小额内存，比如 16 字节的，32 字节的，48 字节的，以及 512 字节的等等，这样程序中就会存在非常多的被 PoolSubpage 对象包装的 8 KB 堆外内存，如果每次申请小额内存的时候，都要从这么多的 PoolSubpage 对象中都查找一遍，岂不是太麻烦了？如果能有一个快速定位要从哪个 PoolSubpage 对象中分配小额内存的方法就好了。所以，在 Netty 中，就为 PoolSubpage 设计了两个数组。一个是 tinySubpagePools 数组，一个是 smallSubpagePools 数组。从名字就能看出来，这两个数组存储的 PoolSubpage 对象的规格是不同的。

如果一个 PoolSubpage 对象包装的 8 KB 内存是被 Tiny 类型的内存均分的，那这个 PoolSubpage 对象就会被放到 tinySubpagePools 数组中，如果一个 PoolSubpage 对象包装的 8 KB 内存是被 Small 类型的内存均分的，就会被放到 smallSubpagePools 数组中。这样一来，这两个数组中可能就会存放多个 PoolSubpage 对象了，每个 PoolSubpage 对象肯定就会对应数组中的一个索引。可是即便是这样，又该怎么快速定位从数组的哪个索引对应的 PoolSubpage 对象分配内存呢？如果想快速定位，势必就要在存放 PoolSubpage 到数组中的时候遵循一定的规范添加。比如说，就比较各个 PoolSubpage 对象中 elemSize 成员变量的值，然后遵循从小到大的顺序添加。如果按照这样的规则，那么被 16 字节均分的 PoolSubpage 对象就该被放到 tinySubpagePools 数组的 0 号索引位置(当然，在 Netty 中并不是直接就放在 0 号位置的，而是放在了 1 号位置。0 号位置存放的是 0)，被 32 字节均分的就要放在 tinySubpagePools 数组的 1 号索引位置，就这样依次向后排列。当然，如果用户想申请 64 字节的小额内存，但是程序内部并没有被 64 字节均分的 PoolSubpage 对象，这个时候就会先去 PoolChunk 中申请一个 8 KB 内存，然后把这 8 KB 内存用 64 字节均分，之后交给一个 PoolSubpage 对象包装，然后就可以从 PoolSubpage 对象中分割出去 64 字节了，而这个 PoolSubpage 对象就会被放到 tinySubpagePools 对应的索引位置。如果用户申请的是 512 KB 的内存，那就从 smallSubpagePools 数组中的 0 号索引查看有没有对应的 PoolSubpage 对象，如果有就直接从该对象中分配小额内存。至于为什么是 0 号索引，因为 Small 类型的内存一共就四种，并且是按照乘以 2 来递增的，第一个就是 512 字节，第二个就是 1 KB，第三个就是 2 KB，第四个就是 4 KB。申请的内存超过 4 KB，就会被规整为 8 KB 了，就要从 PoolChunk 中进行分配了。

逻辑还是一如既往的流畅，但是，我还是想到一个问题，假如说现在用户要申请 16 字节的小额内存，于是来到 tinySubpagePools 数组中查看被 16 字节均分的 PoolSubpage 对象是否能够分配 16 字节的内存。但假如这个 PoolSubpage 对象中的内存都分配完了，不能再分配给用户 16 字节的内存了。按照刚才分析的逻辑，这时候就应该来找 PoolChunk 对象了，从 PoolChunk 对象中分配 8 KB 的内存，然后用 16 KB 把这 8 KB 均分一下，交给 PoolSubpage 对象保管，接着就可以分配 16 字节给用户了。到这里都是之前分析过的逻辑，现在我的问题是，tinySubpagePools 数组中已经存放了一个被 16 字节均分的 PoolSubpage 对象了，现在这个新申请的 PoolSubpage 对象要怎么放到 tinySubpagePools 数组中呢？方法已经很明显了，就是使用 HashMap 的方式，在对应的索引位置以链表的形式连接每一个 elemSize 属性相同的 PoolSubpage 对象。这样如果发现当前节点没有足够分配的内存，就可以查看下一个节点是否有足够的内存。已经分析的差不多了，接下来请大家看下面一幅简图，进一步加深对 PoolSubpage、tinySubpagePools、smallSubpagePools 的理解。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/771bc37df239473ba8046101071c7886~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=2121&h=1485&s=210503&e=jpg&b=fefefe)

PoolSubpage、tinySubpagePools、smallSubpagePools 的结构看完了，在继续讲解之前，我想强调一句，这两个数组的每个索引位置存放的 PoolSubpage 链表的头节点并不参与内存的分配，这个要理清楚。至于经过什么计算能够确定要分配的小额内存应该从哪个数组下标对应的 PoolSubpage 链表中分配，这个就是纯粹的数学计算了。比如说，现在用户申请的是 32 字节的小额内存，肯定是从 tinySubpagePools 数组中分配的，对吧。这个数组中每一个索引对应的值都是按 16 递增的，所以直接让 32 除以 16，得到 2。这样我们就知道应该从 tinySubpagePools 数组的 2 号索引位置开始分配内存。2 号索引位置对应的正好是被 32 字节均分的 PoolSubpage 对象。至于如何确定要分配的小额内存在 smallSubpagePools 数组中的索引下标，这个就是具体的位运算了，源码中有详细的注释，这里就不再讲解了。好了，配图看完了，下面就要从代码层面来继续展示一下这几个类的内容了。首先是 PoolSubpage 类。这个类既然会成为链表的一个节点，所以类中定义的成员变量肯定要增加一些。请看下面代码块。

java

复制代码

`/**  * @Author: PP-jessica  * @Description:这个类的对象就是用来包装8KB内存的  */ final class PoolSubpage<T> implements PoolSubpageMetric {     //该8KB内存是从哪个PoolChunk中分配的     final PoolChunk<T> chunk;     //分配的这块8KB内存在Chunk二叉树数组中的下标索引     private final int memoryMapIdx;     //这个PoolSubpage包装的8KB内存在Chunk中的内存偏移量的大小     private final int runOffset;     //这个PoolSubpage的大小，也就是8KB     private final int pageSize;     //一个PoolSubpage是8KB，最多可以分配成512个16B，所以数组容量最大为     //512，而一个long类型有64位，8个long就有512位，所以用容量为8的long数组就可以有512个索引，也就可以表示PoolSubpage数组中     //哪个位置使用了，哪个位置没有使用，用0和1来区分。这个就是位图     private final long[] bitmap;     //大家应该还记得，在PoolArena中有tiny和small的PoolSubpage数组，这个数组中每一个位置存储的都是     //PoolSubpage链表，所以在这里应该也能明白，从Chunk中分配出的每一个8KB大小的内存，最终都是要加入到     //PoolArena的PoolSubpage数组的对应下标的双向链表中的     //这就是前驱节点指针     PoolSubpage<T> prev;     //下一个节点的指针     PoolSubpage<T> next;     //是否被销毁     boolean doNotDestroy;     //这个属性对应的其实就是用户申请分配的小额内存，经过规整之后的内存大小，因为PoolSubpage会被这个内存大小均分     //比如8KB的内存可以分成两个4KB，也可以分成512个16B，用户申请的是多少的内存，就按规整后的内存把PoolSubpage均分     //加入到PoolArena的时候，也是根据均分之后的elemSize大小，来寻找PoolSubpage数组中对应位置的双向链表加入     int elemSize;     //该PoolSubpage一共分割成了多少个elemSize     private int maxNumElems;     //bitmap数组的容量大小     //这里为什么要有这个属性呢？因为每一个PoolSubpage可能是按照不同的elemSize均分的，所以分成的内存块的数目也不相同     //所以位图的长度也就不同，如果是按照16B分割的，那位图数组的长度就是8，因为要表示512个16B     private int bitmapLength;     //下一个可以使用的内存块在位图中的位置     private int nextAvail;     //该PoolSubpage还剩下的可以分配的内存块数量     private int numAvail; 	//其他内容省略 }`

在上面的代码块中，我只为大家展示了 PoolSubpage 类的成员变量，如果连方法都展示，那要展示的内容就太多了，就成了贴代码讲解了，我无意这么做。在上面列出来的这些成员变量中，我着重要讲解的只有一个，那就是 bitmap。到此为止，我只给大家讲解了从 PoolChunk 中分配出来的 8 KB 堆外内存要交给 PoolSubpage 对象来保管，并且也讲解了这 8 KB 堆外内存要被用户申请的小额内存均分，说得严谨一点，是被规整过的小额内存均分。那么，均分完了之后呢？或者说均分只是一个概念，那在代码层面该怎么表现呢？这时候就该位图登场了。请大家想想，如果用户申请的小额内存是 1 KB 的，那么这 8 KB 就会被均分为 8 个 1 KB。在 PoolSubpage 对象中，就会被包装到 bitmap 数组中，数组的容量是 8，每个位置代表 1 KB 内存。如果这块内存还未被分配，那么索引位置就是 0，如果分配出去了，这个值就会被修改为 1。当 PoolSubpage 对象把小额内存分配给用户时，采用的仍然是老方法，就是用 PoolSubpage 包装的这 8 KB 内存在 PoolChunk 中的内存偏移量，加上要分配给用户的小额内存在 bitmap 数组中的偏移量，把这个总的偏移量返回给用户，加上堆外内存的起始地址，就可以确定 Unsafe 对象真正要操纵的内存地址了。

举一个很直观的例子，假如现在的堆外内存只有 16 字节，也就是一个 PoolChunk 对象只包装 16 字节的堆外内存，要从这 16 个字节中分配 8 个字节交给 PoolSubpage 管理，用来进一步分配小额内存。用户如果申请的是 1 字节的小额内存，那么这个 PoolSubpage 管理的 8 字节堆外内存就要被均分成 8 个 1 字节，用数组 bitmap 来表示。具体情况如下图所示。

![](https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5de21dd8037e4ca8b8ef24e79b771a3f~tplv-k3u1fbpfcp-jj-mark:1890:0:0:0:q75.awebp#?w=2043&h=458&s=37999&e=jpg&b=fefefe)

在上图中有 16 字节堆外内存，但是这 16 个堆外内存的前 8 个字节已经分配出去了，所以只好把后 8 个字节交给 PoolSubpage 对象来管理。PoolSubpage 对象中的 bitmap 成员变量就是一个数组，数组的长度为 8 ，每一个对应 1 字节，每一位的初始值都是 0，表示代表的这 1 字节还没有分配。当用户来申请这 1 字节的时候，bitmap 数组中的前三位都已经分配出去了，所以只好从数组的第 4 位开始分配，也就是索引为 3 的位置。请大家结合着上图来思考，现在这个索引 3 实际上就对应着上图数组中的 索引 11。而 bitmap 数组索引 0 就对应着上图的索引 8。现在我们知道堆外内存的起始地址，还知道索引 8 相对于堆外内存的起始地址偏移量为 8，索引 11 相对于索引 8 的偏移量为 3。3 + 8 得到的就是索引 11 相对于堆外内存起始地址的偏偏移量，而堆外内存的起始地址加上这个偏移量就是索引 11 对应的内存地址。如果用户得到了这个内存地址，Unsafe 对象就可以直接向这个地址写入内存了，1 字节也就交到用户手上了。所以，我刚才说分配小额内存其实就是用 PoolSubpage 包装的这 8 KB 内存在 PoolChunk 中的内存偏移量，加上要分配给用户的小额内存在 bitmap 数组中的偏移量，把这个总的偏移量返回给用户，内存就分配成功了。

说了一大堆，这个和 bitmap 成员变量的关系好像也没有那么复杂，bitmap 就是一个数组，用数组来表示均分后的内存分配情况而已，0 代表未分配，1 代表已分配。这很简单。但是，请大家想一想，如果用户申请的是 16 字节小额内存呢？16 字节来均分 8 KB，会把 8 KB 均分为 512 份，也就意味着 bitmap 数组的容量为 512。看起来挺长的，所以，在 Netty 中就使用了一个简单的数据结构来表示这个数组。我们都已经知道了，数组的每一位其实就是 0 或 1，这正好就是二进制的表现形式。而一个 long 整数为 8 个字节，1 个字节为 8 位，那么 1 个 long 整数就代表 64 位，8 个 long 整数就代表 512 位，正好为被 16 字节均分的 8 KB bitmap 数组的长度，512 位的每一位都可以被赋值为 0 或 1。这样岂不是只用几个整数就实现了这个功能，还节省了内存空间。当然，讲到这里大家肯定也意识到了，bitmap 数组的长度肯定不是固定的，用户申请的小额内存不同，那么均分 8 KB 的内存也就不同，8 KB 最后被均分后的份数肯定也不相同，数组的长度也就不同了。刚才我给大家分析的就是 bitmap 数组最大容量，至于其他的，大家就去源码中学习吧。

好了，这个知识也讲完了，接下来就是 tinySubpagePools、smallSubpagePools 这两个数组了，这两个数组本身没有什么展示的，我这里展示的代码块是 PoolArena 类的内容，因为在 Netty 中，这两个数组就定义在 PoolArena 类中。请看下面代码块。

java

复制代码

`abstract class PoolArena<T> implements PoolArenaMetric {     static final boolean HAS_UNSAFE = PlatformDependent.hasUnsafe();     enum SizeClass {         //Tiny的大小为16B到496B，按照16B的大小递增         Tiny,         //Small的大小为512B到4KB，按照乘以2的大小递增         Small,         //Normal的大小为8KB到16MB，按照乘以2的大小递增，大于16MB的为Huge，不会被缓存，这里的意思是不会被申请为Chunk         //实际上，在Netty的内存分配中，是先从内存中申请了一大块大内，然后再从这一大块内存中逐渐分配给各个线程使用的         //这一大块内存就为Chunk，值为16MB         Normal     }     //512右移4实际上就是512除以16，结果为32，这个值就是tinySubpagePools数组的容量     //因为Tiny是按照16的大小递增的，而它的范围是从16B到496B，递增到496B，一共是31个值，再加上0，就是32个值     static final int numTinySubpagePools = 512 >>> 4;     //所属的内存分配器     final PooledByteBufAllocator parent;     //这个参数就是二叉树的深度，也可以说是高度     //Chunk是一整块内存，但在分配的时候，被我们虚拟成了一个二叉树，叶子结点就是2048个8KB大小的Page，分配也就是     //分配的这些叶子结点，但是这个二叉树的深度会帮助我们快速分配内存，以及计算分配的内存在这块Chunk中的偏移量     //是一个相当重要的属性值     private final int maxOrder;     //每一个叶子结点的大小，8KB     final int pageSize;     //这个值为13，是用来辅助计算所分配的内存大小在二叉树的第几层，也是个非常重要的属性     final int pageShifts;     //Chunk的大小，为16MB     final int chunkSize;     //这个值是个掩码，辅助计算用户申请的内存是大于8Kb的，还是小于8KB的。在具体方法内给大家标记了详细的注释     //大家可以在方法中去学习该值的用法，在构造函数中被赋值，为-8192     final int subpageOverflowMask;     //下面就是这两个数组了          //这个值就是smallSubpagePools数组的容量，值为4，因为smallSubpagePools数组只存放     //512B，1KB，2KB，4KB的内存大小     final int numSmallSubpagePools;     //存放Tiny内存大小的PoolSubpage数组，容量为32，并且数组中的每个位置存储的都是一个PoolSubpage的双向链表     //只有相同规格的PoolSubpage才会存储在同一个双向链表内     private final PoolSubpage<T>[] tinySubpagePools;     //存放Small内存大小的PoolSubpage数组，容量为4，并且数组中的每个位置存储的都是一个PoolSubpage的双向链表     //只有相同规格的PoolSubpage才会存储在同一个双向链表内     private final PoolSubpage<T>[] smallSubpagePools;     //其他内容省略 }`

新引入的这几个成员变量其实也就提醒了我们，在 PoolAena 的 allocate 方法中，其真正分配内存的流程和我之前为大家讲解的是不一样的。真正的流程应该是先根据规整后的内存判断用户申请的内存属于哪种规格，然后根据不同的规格开始分配，而分配的时候肯定是先去内存池中分配，如果内存池中分配不成功，那么就会根据用户申请的是否为小额内存进一步细分。如果是小额内存，并且是 Tiny 规格的，就会去 tinySubpagePools 数组中分配；如果是 Small 规格的，就会去 smallSubpagePools 数组中分配。如果申请的是大额内存，而内存池中又没有分配成功，就会去 PoolChunk 中分配。具体的流程我就不再讲解了，直接给大家贴一张源码吧，源码中添加了详细注释，大家可以看看。这就是我为大家提供的第 14 版本中的源码。请看下面代码块。

java

复制代码

	`/**      * @Author: PP-jessica      * @Description:具体分配内存的方法，这里其实仍然是一些流程步骤，我们之前说了，真正分配内存的方法，是在PoolChunk中进行的      */     private void allocate(PoolThreadCache cache, PooledByteBuf<T> buf, final int reqCapacity) {         //这个方法就起到了规整要申请的内存的作用         //得到规整之后的内存，所以，这里大家也可以清楚了，并不是用户申请的多少内存就分配多少         //分配系统会自动补全要分配的内存         final int normCapacity = normalizeCapacity(reqCapacity);         //判断要分配的内存是多大，是不是tiny或者small大小的         if (isTinyOrSmall(normCapacity)) {             //走到这里说明是tiny或者small大小的，要继续细分             int tableIdx;             PoolSubpage<T>[] table;             //判断要申请的内存是否为tiny范围大小的             boolean tiny = isTiny(normCapacity);             if (tiny) {                 //走到这里说明是tiny大小的                 //先从内存池中分配，分配成功就直接返回，不成功就继续向下执行，内存池分配内存的逻辑后面具体再看                 if (cache.allocateTiny(this, buf, reqCapacity, normCapacity)) {                     //大家可以看到，这里实际上是没有返回值的，还记得我们之前从线程私有的对象池中获得的ByteBuf吗？                     //上面的方法中了，这进一步说明用户申请的内存实际上是被包装到ByteBuf中了，并不是说ByteBuf就是一块内存                     return;                 }                 //内存池分配不成功就会走到这里                 //这里把要分配的内存传进去，得到一个数组的下标，实际上就是tinySubpagePools数组的下标                 //这个下标，意味着要分配的内存在tinySubpagePools数组的哪个位置                 //因为tinySubpagePools数组内的下标对应的是[0，1，2....31]，以16递增的                 tableIdx = tinyIdx(normCapacity);                 //把tinySubpagePools数组赋值给上面定义的那个table                 table = tinySubpagePools;             } else {                 //走到这里说明要分配的内存大小在small范围内                 if (cache.allocateSmall(this, buf, reqCapacity, normCapacity)) {                     //同样是在内存池中分配成功后就可以直接返回了                     return;                 }                 //内存池分配失败，则计算要分配的内存在smallSubpagePools数组中的下标位置                 tableIdx = smallIdx(normCapacity);                 //把smallSubpagePools数组赋值给上面定义的那个table                 table = smallSubpagePools;             }             //这里得到了双向链表的头节点，索引在上面确定了，无论是从smallSubpagePools数组还是从tinySubpagePools数组分配内存             //都要先得到数组下标的头节点，头节点不参与分配的             final PoolSubpage<T> head = table[tableIdx];             //加锁，并且是以头节点为锁的。注意，我们说内存池也是每个线程私有的，但这里并不是从内存池中分配，是多个线程             //从这个PoolArena中申请内存，所以就要考虑并发问题，自然要加锁             synchronized (head) {                 //得到头节点的下一个节点                 final PoolSubpage<T> s = head.next;                 //我们之前说过，头节点不参与内存分配，所以如果下一个节点就是头节点，说明该链表没有可以用来分配的PoolSubpage节点                 //因为PoolSubpage中的内存分配完了，该节点就会从链表中删除                 if (s != head) {                     //这里是判断一下PoolSubpage对象没被销毁，并且PoolSubpage是以normCapacity内存大小分割的                     //这里大家可能还不太懂什么意思，我们到了PoolSubpage类中会继续讲解，到时候大家就会发现串起来了                     assert s.doNotDestroy && s.elemSize == normCapacity;                     //从PoolSubpage节点中分配内存，具体的方法在PoolSubpage类中                     //得到一个内存偏移量                     long handle = s.allocate();                     assert handle >= 0;                     //初始化ByteBuf，实际上就是把申请的直接内存交给ByteBuf包装，具体逻辑我们后面再看                     s.chunk.initBufWithSubpage(buf, null, handle, reqCapacity);                     //分配的tiny内存次数加1或者是small内存的次数加1                     incTinySmallAllocation(tiny);                     return;                 }             }             //走到这里说明上面从PoolSubpage节点中分配内存失败了，可能是根本就还没有PoolSubpage节点可供内存分配呢             //所以，这里要从Chunk中分配内存。             //这里大家是不是也可以把之前的逻辑串起来了？判断内存分配的大小，如果太小，就先从tiny或者small数组中分配小内存             //如果数组分配内存失败，才会从Chunk中分配。但是大家还要记住，从Chunk中分配出来的，实际上还是一个PoolSubpage对象，这个队形             //会添加到PoolSubpage数组中相应的链表中，这个逻辑到后面也会串起来，这里大家先记住就行             synchronized (this) {                 //上面分配内存失败，所以这里就要直接从Chunk中申请一个8KB的内存空间                 //这块内存实际上会交给PoolSubpage包装，而PoolSubpage会添加到数组的链表中                 allocateNormal(buf, reqCapacity, normCapacity);             }             incTinySmallAllocation(tiny);             return;         }         if (normCapacity <= chunkSize) {             //走到这里说明分配的内存是大于8KB小于16MB的             //先从内存池中分配内存             if (cache.allocateNormal(this, buf, reqCapacity, normCapacity)) {                //分配成功直接返回                 return;             }             //走到这里说明没有从内存池中分配成功，就会从ChunkList中尝试分配内存             synchronized (this) {                 allocateNormal(buf, reqCapacity, normCapacity);                 ++allocationsNormal;             }         } else {             //走到这里则说明要分配的内存是大于16MB的，这时候就要直接申请一个Huge块的内存，这个内存用完就被释放了             allocateHuge(buf, reqCapacity);         }     }`

到此为止，Netty 的内存池的知识我就给大家全部讲解完毕了。说实话，我写文章已经写得很累了，这几篇文章接连写了有半个月，写得确实也啰嗦，但是我的初衷是写得尽可能详细，一个知识点翻来覆去的讲解，就是希望能真正让大家学会，看懂。如果哪里写得不对，或者写得不仔细，不清晰，大家可以直接加我微信私聊我，问我，给我指正，各种建议我都会虚心接受。请大家多多包涵一下了。

**总结**

这一篇确实有赶工的痕迹，这一点我必须承认。因为不得不为大家更新文章了。我当然不是抱怨大家在催我，因为我确实应该更新了，我只是觉得自己文章写得太长了，也许本来不用写这么长的，是我自己没有把控好篇幅长度。其实我本可以一篇一篇给大家更新，但我不愿意那么做，因为篇幅很长，如果每星期更新一篇，下星期更新文章时，可能之前看过的就已经忘了，到时候大家还要往前翻看、温习。最好的方式就是一次就更新完，这样大家学起来也过瘾。而且我也不喜欢写作思路被打断，我觉得这块知识是一个整体，就想不间断地写下去，直到写完。哪怕会写半个月，也要天天写，写完为止，而且我写的过程中发现哪里不完善，还会回过头修改前面章节的某些地方。因为这些知识本就是紧密连接在一起的。

本来这一章我不想写得这么急，这么赶，其实很多内容都可以展开来讲解，比如一点点剖析和迭代数组表示二叉树的过程，内存偏移量在代码中的计算过程是什么等等，但是内存池这一块已经耗费了很多时间了，并且大家也希望我尽快更新。所以就先写成这样吧，好在核心流程都迭代完整了，内存分配的过程以及防止内存重复分配的方法也讲清楚了。所以，剩下的，就交给大家自己去源码中结合注释学习吧。

至于最后一个知识点，就当作彩蛋放到这里吧，不知道大家有没有担心过内存清零的问题，如果使用过的内存直接归还并且不做清零或者说初始化操作，下一次分配出去，用户读到了没有被清零的垃圾数据怎么办？这一点其实不必担心，别忘了，ByteBuf中有读写指针，读取和写入数据都是根据指针来的，写到哪里，就会读到哪里。如果读取数据的时候，还会判断要读取数据的长度加上读指针的长度是否大于写指针的长度了，如果超过了就会抛出异常，所以读取的都是有效数据。