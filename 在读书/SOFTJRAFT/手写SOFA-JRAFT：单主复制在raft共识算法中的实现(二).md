大家好，在上一章的结尾，我为自己的程序引入了 LogEntry 这个类，并且打算再次重构一下自己的程序。所以就不说什么废话了，直接开干。首先我们先来看一下这个 LogEntry 类的具体实现。请看下面代码块。

```
//该类的对象就是一个日志条目
//这里我多解释一句，大家应该意识到，在程序完整的执行流程中需要经过几次编解码
//首先客户端向服务端发送指令，会经过一次编解码，主节点向从节点发送日志条目也会经过一次编解码
//在我们这个不完善的例子中，客户端向服务端发送指令时会对Task对象进行编解码
//当主节点向从节点发送日志条目时，会对LogEntry对象进行编解码。这个过程一定要理清楚
//当然，在我们的例子中我也没提供什么编解码功能，RPC功能也没有，都会写成伪代码
//但我就是想提醒大家一下，因为后面的章节中还是要从零开始完整实现所有功能的
public class LogEntry {

    //日志索引
    private long index;

    //封装的指令信息
    private Task task;

    public LogEntry() {
        
    }

    public LogEntry(long index, Task task) {
        this.index = index;
        this.task = task;
    }

    public long getIndex() {
        return index;
    }

    public void setIndex(long index) {
        this.index = index;
    }

    public Task getTask() {
        return task;
    }

    public void setTask(Task task) {
        this.task = task;
    }
}
```

LogEntry 类已经定义好了，类的内容和逻辑非常简单，就不再详细解释了，接下来，我们看看 NodeImpl 类该怎么重构。请看下面代码快。

```
//节点实现类
public class NodeImpl{

    //这个成员变量记录的就是我一共挣到了多少钱
    private Integer totalMoney;
    
    //记录每一个私人物品藏在哪个位置了
    private Map<String,String> map = new HashMap<>();
    
    //当前节点的状态，是主节点还是从节点
    private State state;
    
    //记录集群中所有节点IP地址的成员变量
    List<Endpoint> list = new ArrayList<>();
    
    //记录集群中主节点的IP地址
    Endpoint master;
    
    //当前节点自己的IP地址
    Endpoint ip;
	//最后发送或者是接收到的指令的索引
    private long lastIndex;


    public NodeImpl() {

    }

    //构造方法
    public NodeImpl(State state, List<Endpoint> list, Endpoint ip) {
        this.state = state;
        this.list = list;
        this.ip = ip;
    }

   //其他方法省略

    //应用客户端指令的方法，客户端发送过来的指令会被这个方法处理
    //假装已经经过反序列化了，这里直接就可以得到反序列化之后的Task
    public void onApply(Task task) {
        if (this.state == STATE_MASTER) {
            //直接开始执行指令
            execute(task);
            //执行完指令之后把指令发送给从节点
            sendCommandToSlave(task);
        }
    }


     //主节点向从节点发送消息的方法,这个方法做了一些简单的修改
    private void sendCommandToSlave(Task task) {
        //这里发送指令的时候，要先给lastIndex自增，
         this.lastIndex++;
        for (int i = 0; i <list.size() ; i++) {
            Endpoint endpoint = list.get(i);
            if (endpoint.equals(ip)) {
                //如果当前遍历到的节点信息和当前节点信息相等，就直接跳过本次循环
                //不可能自己给自己发送消息
                continue;
            }
            String ip = endpoint.toString();
            //在这里把指令包装成LogEntry对象，日志索引也设置了
            LogEntry log = new LogEntry(lastIndex,task);
            //把日志发送给从节点
           doSendCommandToSlave(log,ip);
        }
    }


    //真正发送指令消息给从节点的方法
    //这里需要解释一下，当主节点发送指令失败，要重新发送指令时
    //就直接调用这个方法，这时候index并不会自增了，还是发送失败的指令对应的index
     private void doSendCommandToSlave(LogEntry log,String ip){
         //暂时不做实现
     }

    

    //从节点接收主节点传递过来的指令的方法
    //这个方法也做了一点改动
    private Response acceptCommand(LogEntry log) {
        //如果当前节点是从节点，就开始执行指令
        if (this.state == STATE_SLAVE) {
            if(log.getIndex() == lastIndex + 1){
                //更新lastIndex
                this.lastIndex++;
                //执行指令，执行指令的时候可以异步将日志持久化
                execute(log.getTask());
            }
            return response;
        }
        return null;
    }



    //省略其他方法
}
```

现在程序又一次重构完成了，可以看出，具体改动的地方并不多，逻辑仍然非常简单。引入日志这个概念，同时也引入了 LogEntry 类之后，主节点和从节点数据的备份和一致就全依靠日志来保持了。用 LogEntry 对象传输操作指令，持久化日志也就做到了操作指令持久化，只要每一条日志都持久化成功了，那么不管哪个节点宕机，节点重启之后，**数据也不会丢失。**这样看起来不错，但我还是忍不住问一句，要是日志持久化失败了呢？

还记得上一章结尾，我为大家提供的那个程序执行的流程简图吗？我把那张简图搬运到这里了，并且给每个执行步骤添加了标号。

![](https://cdn.nlark.com/yuque/0/2023/jpeg/26725125/1703763251788-e1b0dda0-8b03-4307-85d2-4f1973ec6085.jpeg)

在上面的简图中，步骤 4 和 步骤 5 是同步进行的，步骤 7 和步骤 8 是同步的。我在此解释这个是有用意的，还是回到刚才的问题，**要是日志持久化失败了呢？确切地说，是日志在从节点持久化失败。**也许有朋友不明白我的意思，让我来简单解释一下，**我说的日志持久化失败，指的是主节点接收到客户端传来的指令后，先应用到了主节点，并且把指令封装为日志条目对象，但是还没来得及传输给两个从节点，发生了网络分区，主节点这时候无法与其他节点，甚至是客户端都不能再联系了。**这时候，对于从节点来说，就接收不到主节点传输过来的日志，也就不可能持久化日志条目，更别说应用日志条目中的指令了。当然，主节点出现故障了，我可以很快从那两个从节点中手动选取一个新的主节点，然后让客户端向新的主节点发送操作指令。那这时候我就想问一句了，这么做会不会出现什么差错呢？

**分析主节点应用指令和传输日志操作****的顺序**

好了，让我先来跟大家说一下结论，差错肯定是会有的，但现在还不到具体分析差错的时候。先让我为大家引入另一个概念，**那就是客户端指令的提交。**这句话的意思很简单，客户端访问集群的主节点，向主节点发送了一条操作指令，比如说希望 NodeImpl 的 map 中添加一个键值对，客户端会将这个操作指令发送给主节点，主节点会先应用这条指令，然后把这条指令包装成日志对象发送给从节点。注意，**这时候主节点已经成功应用了这条指令，所以，确实有理由可以让主节点向客户端回复一个成功响应，客户端只要收到了这个响应，就代表它发送的这条操作指令已经被成功应用了，或者已经被提交了。如果主节点迟迟没有给客户端回复成功响应，那客户端可能也要实现一些超时重试的功能，如果什么也不做，那这个指令操作不就丢失了吗？**所以，在上面的那副简图中，在步骤 3 的位置，应该再多出来一个分支，当主节点成功应用了指令之后，回复给客户端一个响应。在 NodeImpl 类的代码中，也应该对 onApply 这个执行指令的方法进行一些重构。但这里我就不再重构了，大家理解这个意思就行。接下来让我们再回到之前那个问题：主节点接收到客户端传来的指令后，先应用到了主节点**，回复给客户端一个指令应用成功的响应**，并且把指令封装为日志条目对象，但是还没来得及传输给两个从节点，就发生了网络分区，主节点这时候无法与其他节点，甚至是客户端都不能再联系了，然后我从剩下的两个从节点中挑选出了一个新的主节点，让客户端继续把下一条指令发送给新的主节点。这时候可能就会发生数据冲突的情况。

请看下面的简图。

![](https://cdn.nlark.com/yuque/0/2023/jpeg/26725125/1703829235413-fc08c5a7-358f-48f1-9ec2-8d00a2c7efd4.jpeg)

在上面的简图中，集群中有三个节点，节点 1 为主节点，节点 2、3 分别为从节点。客户端首先向主节点发送了一条 map.put("1000元零花钱","藏在枕头套里") 指令，主节点给客户端回复了成功响应，然后主节点就发生了网络分区，指令根本就没有传输到从节点，而是被包装成日志在本地持久化了，日志索引为 1。这时候两个从节点还没有一条日志呢，新的主节点被我选举出来之后，客户端向主节点 2 发送了一条 map.put("1000元零花钱","被媳妇发现了，别再惦记了") 指令，把 "1000元零花钱" 这个 key 对应的 value 修改了，并且顺利传输给从节点 3。这时候主节点 2 和从节点 3 的 map 数据是一样的，本地持久化的日志索引都为 1。而从网络分区故障中回复的节点 1 也作为从节点重新加入集群了，这时候节点 1 的 map 还是旧数据，本地持久化的日志索引也为 1。这样一来，集群中不同节点就发生了数据冲突，如果要更进一步拓展，主节点只负责处理写操作，而从节点负责处理读操作，那这时候客户端从节点 1 和节点 3 得到的数据是完全不同的。这就很尴尬了。**如果从这种情况来分析的话，显然，不应该让主节点先应用指令，再传输日志，否则当集群发生网络分区，主节点变更后，可能出现数据冲突的情况，主节点和从节点的数据不一致，数据备份的效果也就无从谈起。**

当然，也许有朋友会说，之所以会发生上面那种情况，是因为主节点自作主张，它光顾着自己了，只要自己应用指令成功就响应客户端，而不管其他从节点有没有执行成功。就像你们公司最近效益不好，老板打算降低成本，每个员工降薪 1000，你们还没发表意见呢，主管直接点头同意了。告诉我，这时候你是不是很想问候主管的家人？那就别让主节点自作主张了，让主节点看看每一个从节点是否应用指令成功了，换句话说**，就是当每一个从节点应用指令成功后，都回复了主节点成功响应，再让主节点回复客户端执行应用成功的响应，这样不就能使每个节点状态一致了吗？**这种做法看起来好像没什么问题，实际上根本经不起推敲，就举一个最极端的例子吧，假如客户端发送了一条指令，不管是主节点还是从节点，都成功执行了这条指令，然后回复给客户端指令执行成功了。客户端就不再关心这条指令了，以为指令操纵的数据成功保存并且备份了。但是，集群中的各个节点仅仅只是成功应用了这条指令，如果都没有将指令持久化到本地，那么集群中的节点都宕机了，这条指令不就丢失了吗？就算重新启动节点，指令并没有被包装成日志持久化，就意味着节点无法执行指令中的操作，节点的状态也就无法回复到宕机之前。但是客户端一直以为自己执行成功了，就不再关心刚才发送的指令了。

而本来客户端给主节点发送一条指令，如果一直没有收到主节点成功的响应，根据失败重试机制，或者发现集群中产生了新的主节点，说不定就会把这条指令发送给新的主节点了，总之，这条指可能根本不会丢失，操作操作的数据也不会丢失。但是主节点自作主张之后，或者集群中所有节点仅仅是把指令应用成功了就回复成功响应，这在某些极端情况下，很可能会导致数据丢失。比如说你明明刚刚才某个商城买了一个东西，页面已经收到下单成功的响应了，也展示给你了。但商城的服务器宕机之后，你的购买记录没了。对你来说，操作明明已经成功了，但是数据还是丢失了，这种情况不管是对用户，还是对商家，程序开发者来说，都是不可接受的。该怎么解决这个问题呢？也许大家都已经发现了，我在上面列举的两种出现差错的情况，都有一个共同点，那就是指令明明已经提交了，可是集群中还是出现了数据冲突，节点与节点之间的状态不一致，或者是已提交的指令操作的数据丢失等等问题。我不希望看到这种情况，**我希望的是一条客户端指令一旦提交，就意味着这条客户端指令操作的数据就永远不会丢失，并且集群中每个节点中数据完全一致，也就是节点状态完全一致。**只要实现了这个机制，那上面遇到的问题就再也不会出现。那这个机制该怎么实现呢？

讲到实现就容易很多了，因为上一章已经为大家仔细分析了，**要想让数据不丢失，就要对每一条日志进行本地持久化，因为客户端指令被包装成一条条日志了，只要主节点传递给从节点的每一条日志在集群中的所有节点都持久化成功了，这就意味着各个节点的数据肯定就不会丢失了，因为即便是节点宕机了，重启之后，也可以执行本地每一条日志包装的指令，使节点恢复到宕机之前的状态。并且，我们还可以进一步发现，只要每一条日志都从主节点顺利传输到从节点，那么每个节点之间的状态肯定也都是一致的，就算某个时间段不一致，通过应用各自接收到的日志，最终也会达到一致的状态。**

**也就是说，要想保证数据不丢失，保证每一条已经提交了的客户端指令都有效，就要等到这条指令被包装成日志之后，这条日志被集群中各个节点持久化完毕，再向客户端回复成功响应，这样一来，就可以保证只要每一条已经确认提交了的指令，其操作的数据肯定不会丢失，以及集群中各个节点的状态都会保持一致。**

如果是这样，那是不是就应该等到集群中的每一个节点都应用了日志，然后把日志持久化了，最后再让主节点回复客户端成功响应吗？如果真有人这么想，那我就真的想说一句卧槽了，为什么就非要先应用指令呢？就想和指令干起来是吧？上面分析了那么多，这时候大家应该意识到了，**关键之处不在于什么时候应用指令，因为即便应用了指令，也可能因为日志没有持久化，造成数据丢失或者集群节点之间数据备份不一致，状态不一致。所以，我们要做是什么，应该先让集群之间的节点对某个操作，或者说对某条日志达成一致或者是共识，所谓达成共识，也就是各个节点的日志持久化完成，达成了一致就意味着可以提交这条日志了，然后再应用指令，这样一来，只要被应用了的指令，其对应的数据就一定不会丢失，因为都已经持久化了；只要被应用了的指令，每个节点肯定都拥有这条指令对应的日志，按顺序应用每一条指令，那么集群中每一个节点的数据都是一致的，状态也是一致的。**

**所以，最后就达到了一种效果，客户端发送给主节点的每一条指令，只要客户端收到了已提交的成功响应，那这条指令操作的数据就一定不会丢失，节点之间的状态也一定一致；至于没有收到成功响应的指令，那数据丢失了也无关紧要。**就好像用户下单买商品，下单的操作成功了，那用户下单的数据就不会丢失，也不应该丢失，如果出于各种原因，下单操作迟迟没有成功，也就是客户端一直没有收到主节点的成功响应，那就随便吧。还是那句话，网络是波动的，节点也是有可能宕机的，我们不可能保证每一个操作都能执行成功，也不可能保证每一条数据都成功备份和持久化，但我们可以设计一种机制，只要指令应用成功了，那么与指令对应的数据就不会丢失，数据在各个节点的备份都保持一致，节点状态也会保持一致。

讲到这里，我相信已经把围绕着日志持久化的相关知识剖析得比较透彻了，但我还是想再啰嗦几句。刚才我说的要让集群中的所有节点先对某条日志达成共识，然后再应用指令。所谓的达成共识，也就是主节点把接收到的客户端指令包装成一条日志，然后把这条日志同时发送给集群中的所有从节点，每一个从节点把这条日志本地持久化成功了，**也就是日志复制成功了**，都回复给主节点一个成功的响应，这时候，我们就可以称集群中的所有节点对某条日志，也就是日志包装的指令达成了共识。然后再让主节点应用这条指令，这就是我为大家总结出来的要点：**先达成共识，再应用指令。**等主节点成功应用了这条指令，就可以直接给客户端回复一个成功响应了，或者说通知客户端指令应用成功。这时候，我在文章最开始为大家提供的那张简图，就需要做一些变动了，具体改动如下。

![](https://cdn.nlark.com/yuque/0/2023/jpeg/26725125/1703935381701-816199dd-6e1c-46ab-85a6-7407278762ed.jpeg)

到此为止，我相信上面的这个简图，已经把主从节点数据保持一致，节点状态保持一致，数据不丢失的机制流程展示清楚了。课程进行到这里，如果说我们追求的就是和数据备份相关的功能实现，那么到这里其实已经讲解的差不多了，理论方面差不多已经无懈可击了，要细节有细节， 要概括有概括，总之就是先在各个节点达成一致，然后再应用，而达成一致就是各个节点都复制日志成功，也就是日志在本地持久化成功。但是，没错，我还是想说一句但是，既然是开发程序，难免要考虑得全面一些。按照上图展示的流程，主节点必须等待所有从节点将日志都持久化成功了，并且收到成功响应了，主节点才可以应用这条指令，然后回复给客户端一个成功响应。在我看来，持久化这个操作本身就是比较耗时的操作，但这个不是我们要关注的重点，因为等待一个节点持久化，和同时等待多个节点持久化，在网络没有波动的情况下，等待的时间也差不了多少。**但是，假如网络有波动呢？这时候，等待一个节点完成持久化的时间，和等待多个节点完成持久化的时间，可能就相差很多了。因为一个集群中从节点越多，那么主节点向从节点复制日志时就越可能出现网络问题，可能日志传输得比较慢，或者是从节点回复的响应在途中耽搁了。如果一个集群中有一个主节点，26 个从节点，有任何一个从节点回复主节点不及时，主节点就要多等待一会儿。总之，我现在觉得，让主节点等待集群中所有从节点回复了正确响应，也就是等待所有从节点持久化日志成功，再提交对应的日志，也就是应用这条日志包装的指令，我认为这么做对程序的性能来说是一种拖累。**那有没有什么可以优化的手段呢？

**日志持久化冗余机制、从节点应用日志时机**

当然有优化的手段，但是，在讲解优化方法之前，我们先来探讨一个问题，上一章讲解了那么多内容，这一章也讲解了很多内容了，我们所讲的一切内容，最终目的都是为了数据能够备份，引入了日志复制机制，主节点和从节点拥有相同的日志，将日志本地持久化，这样一来，就能达到我们想要的效果。**只要日志持久化成功，就算一个节点暂时没有成功应用这条日志包装的指令，但最终肯定也会应用成功的。**毕竟节点本身已经拥有这条日志了，只要按照日志索引的顺序依次执行每一条日志中的指令，最终也可以达到和主节点相同的状态。**而从节点的日志都是从主节点复制过来的，这也就是说，只要主节点拥有这条日志，并且没有网络故障，那从节点就一定会复制成功这条日志。**可问题是，如果主节点出现故障了呢？比如说，集群中有 1 个主节点，4 个从节点，主节点从客户端收到了一条指令，主节点把这条指令包装成了索引为 1 的日志，然后复制给其他 4 个从节点，但是传输的过程中，可能发生了网络波动，这条指令只传输给了从节点 2、3，剩下的从节点 4、5 并没有接收到这条索引为 1 的日志。这时候，主节点宕机了。这样的话，整个集群中就只有从节点 2、3 拥有这条日志，而从节点 4、5 并没有这条日志。请大家注意，主节点宕机并不意味着整个集群就瘫痪了，否则集群也就没有存在的必要了。之所以建立集群，首先当然是为了数据的备份，其次就是提供一定的容错机制。也就是说，当主节点宕机的时候，可以立刻从剩下的从节点中挑选一个节点作为新的主节点，这样整个集群仍然可以对外提供服务。那么，现在我想问问大家，我希望这条索引为 1 的日志包装的指令能够执行成功，我应该怎么做呢？

显然应该先选取新的主节点，那么新的主节点该怎么选择呢？现在集群中还剩下 4 个节点，其中节点 2、3 拥有索引为 1 的日志，节点 4、5 并没有这条日志。**现在我又希望这条日志可以应用成功，而日志又是从主节点传递给从节点的，所以，我选择新的主节点，肯定应该从节点 2、3 中选择，因为 2、3 节点拥有这条日志。比如说就选择节点 2，节点 2 当选主节点后，会将自己的拥有的日志传输给从节点，传输给节点 3 的时候，节点 3 发现自己已经有对应的日志了，就不会接收这条日志，可以忽略日志，并直接回复主节点成功响应；传递给节点 4、5 的时候，节点 4、5 还没有索引为 1 的日志，所以会接收这条日志，持久化成功之后，会回复主节点成功响应。**

这时候大家是不是意识到了，好像主节点也不用等待全部从节点持久化完成，才能向客户端回复指令应用成功的响应。因为在我们刚才的例子中，主节点只把日志传输给了两个从节点，而且主节点还宕机了，即便如此，我手动选出了包含索引为 1 的日志的新主节点，日志也继续复制给了剩下的从节点。这么一看，日志也没有丢失，数据也没有丢失，反正其他从节点也拥有日志了，持久化也完成了，只要应用了指令，那么集群中各个节点的状态就都一样了。**如果更进一步分析的话，好像主节点其实只需要把日志成功复制给集群中任何一个从节点就行，只要这个从节点持久化日志成功了，给主节点回复响应，也就是说，主节点只需要接收到来自集群中的任何一个从节点的成功响应，就可以直接应用日志，然后把指令应用成功的响应返回给客户端就行了。**因为在这种情况下，假如主节点并不会宕机，也不会发生网络波动，那么主节点总会收到所有从节点的成功响应；如果主节点宕机了，那么我就可以直接将集群中唯一复制日志成功的从节点选为新的主节点即可，这样主节点就会继续把索引为 1 的日志复制给其他从节点了。

**由此可以推断，主节点根本不需要等到集群中所有从节点都持久化日志成功后再应用指令，然后给客户端回复响应，只需要接收到集群中任何一个从节点复制日志成功的响应，就可以直接回复客户端指令应用成功了，**这样可以吧？这时候我就不得不说一句，很遗憾，根本不能这么做。原因很简单，首先，并不是所有节点都对这条日志达成共识了，再回复客户端指令应用成功的响应；其次，假如说唯一复制日志成功的从节点也宕机了，该怎么办呢？剩下的从节点根本就没有索引为 1 的日志啊，这样一来，在主节点和那个拥有索引为 1 的日志的从节点重启之前，集群就不工作了吗？集群肯定还是要继续工作的，但主节点宕机之前已经收到了唯一一个日志复制成功的响应，于是主节点应用了日志，并且给客户端发送了应用成功的响应。当从剩下的没有日志的节点中选出新的主节点后，在旧的主节点和拥有日志的从节点宕机期间，集群中的主节点以及从节点都不可能拥有索引为 1 的日志了，也就无法执行对应的指令。**这时候，如果有用户来访问数据，明明刚才已经应用成功了，结果在集群中的节点里根本没有这个数据，这就在一定程度上造成了数据丢失。而客户端也许会继续发送新的指令，这时候新的指令就会被新的主节点包装成索引为 1 的日志，持久化到本地，并且复制给其他从节点。这样一来，当旧的主节点和那个从节点重新启动后，它们也拥有索引为 1 的日志，如果日志包装的指令并不相同，这就造成了数据冲突。**总之，上面这种方式根本行不通。

也许有朋友会觉得我总是假设宕机，或者网络故障，如果总是假设宕机或者网络故障，那就没有任何一个能让集群正常提供服务的情况了。确实，我们不能总是假设某个“关键”节点宕机，正如不能总是假设集群中所有节点突然宕机一样，也应该定义一个宕机的程度。然后根据这个宕机的程度，再去分析该怎么实现**所有节点先达成共识，然后再应用日志的机制。**

那么这个宕机的程度该怎么定义的？换句话说，我们在分析集群运行状况的时候，最多可以假设集群中同时有几个节点发生故障呢？毕竟我们之后是要在这个基础上，分析主节点复制日志给几个从节点之后，就可以认为从节点对这条日志达成共识，从而提交这条日志了。**在开始分析问题之前，首先我们一定要意识到，不管有几个节点宕机，我们最终的目的是为了让多个节点宕机后，某一条已经被应用的日志，其操作的数据不会丢失，也不会在其他节点重启之后，集群中发生数据冲突的情况。简而言之，一旦所有节点对某些日志达成了共识，就应该允许集群在部分节点故障的情况下，继续提供正常的服务。**

这时候，我们就应该回到“共识”本身上来了。所谓共识，说到底也就是少数服从多数，因为不可能所有人意见都相同，正如集群中所有节点可能都处在不同的网络环境中。在这种情况下，只能是少数服从多数。如果是这样的话，当有 6 个人的时候，不可能达成少数服从多数的情况，3 ：3 就会造成无解的局面。所以，一定是在奇数个人的时候，才能达到少数服从多数的局面。类比到集群中，也就是集群中只有奇数个节点时，才能达到少数服从多数的局面。

现在我可以为大家举一个很详细的例子了，假如我们的集群有 5 个节点，1 个主节点，4 个从节点，**达成共识，就意味着集群中大多数节点都同意执行某一条指令，也就意味着大多数节点都已经把包装这条指令的日志持久化到了本地。但是根据少数服从多数原则，主节点不必再等待所有从节点都把这条日志持久化到本地，才能决定这条日志是否该提交，也就是这条日志包装的指令是否可以应用了。按照少数服从多数原则，主节点只需要把日志成功复制给集群中的 3 个节点，就可以决定应用这条日志包装的指令，然后回复客户端指令应用成功的响应了。这样一来，主节点本身是拥有这条日志，它只需要把这条日志成功复制给另外两个从节点，并且收到接收都日志的两个节点持久化成功的响应，即可决定提交这条日志，给客户端回复成功响应。**

如果我们真的这么做了，现在集群中的 5 个节点，分别为节点 1、2、3、4、5。其中节点 1 为主节点，节点 2、3、4、5 为从节点。主节点 1 和从节点 2、3 都拥有索引为 1 的日志。而从节点 4、5 还没拥有这条日志，或者是日志还没有在本地持久化成功。总之，现在集群中已经有三个节点对索引为 1 的日志持久化成功了，所以主节点应用了这条日志，并且回复了客户端指令应用成功响应。很好，这就是少数服从多数的局面，我们已经实现了。现在让我们来分析一下，如果是这种情况下，最多宕机几个节点，能够保证集群中已经应用的指令，其操作的数据不会丢失。

假如集群中有一个节点宕机了，比如说，最坏的情况，就是主节点宕机了，那我可以从拥有索引为 1 的日志的节点 2、3 中选出一个新的主节点。这样一来，就算从节点 4、5 还没有索引为 1 的日志，那么选举出来的新的主节点仍然可以把索引为 1 的日志复制给节点 4、5，最终，集群中所有节点的状态仍然会达成一致，数据并不会丢失。那如果集群中宕机了两个节点呢？就比如说主节点和节点 1 宕机了，那集群中还有节点 2 拥有索引为 1 的日志，我直接把节点 2 选为新的主节点，这样一来，就可以让主节点把日志继续复制给节点 4、5 了。

但假如集群中三个节点宕机了呢？最坏的情况就是主节点和从节点 2、3 都宕机了，而这时候指令明明已经成功了，如果集群继续提供服务，就会出现数据丢失的情况，因为节点 4、5 根本就没有索引为 1 的日志。如果在其他三个节点重启之前客户端继续向集群发送新的指令，那三个节点重启后就有可能出现数据冲突的情况。

**所以，综合分析下来，我发现如果集群中有 5 个节点，我们把允许宕机的最大节点数限制在 2 以内，也就是最多允许 2 个节点出现宕机的情况，那么主节点就可以在日志复制到集群中 3 个节点成功之后，就可以立刻提交这条日志了。因为这样操作下来，就算是两个节点宕机了，总还是有一个节点拥有索引为 1 的这条日志，将这个节点选择为新的主节点，它就可以继续把日志复制到剩下的的节点上了。这样一来数据既不会丢失，其他节点重启之后也不会出现数据冲突的情况。最重要的是，剩下的 3 个节点仍然可以对某条指令达成共识，集群中的 5 个节点，宕机了 2 个，并且最多只允许宕机 2 个，剩下 3 个可以一直正常运行，那剩下 3 个节点会成功将每一条日志本地持久化，就算那 2 个宕机的节点一直无法恢复，集群中的日志也会一直以 3 : 2 的票数达成共识。**这也是我们愿意看到的情况。

按照这种规则，我们可以进一步分析一下，如果集群中有 7 个节点呢？如果是这样，按照少数服从多数的共识原则，主节点只需要把日志成功复制到 4 个节点上就可以提交这条日志了。这时候，我就没必要再啰嗦了，可以直接为大家总结一下：**在构建分布式集群中，我们可以在集群中创建 2n+1 个节点，节点个数是奇数，整个集群最多允许 n 个节点同时出故障。在这个前提下，集群的主节点只需要把包装了指令的日志成功复制给 n+1 个节点，n+1 节点中也包含主节点本身，这时候就可以决定应用这条日志包装的指令了。当然，要想使这种情况成立，必须严格执行一个关键步骤，那就是在选举新的主节点时，一定要选举拥有最新日志的节点为主节点，或者是选举出来的主节点的日志比其他节点的新。如果主节点没有对应的日志，刚才为大家总结的情况仍然无法成立。而这个就是日志在集群节点中复制的冗余机制。**

到此为止，我想我们的课程也讲解的差不多了，是时候为大家简单总结一下了。虽然之前讲解了这么多的内容，看起来好像讲了太多知识，但实际上我们的目的非常简单：**就是希望服务器中的数据可以被完美备份，集群中备份数据的从节点和主节点的状态保持一致，数据保持一致；并且还希望只要客户端的一条指令应用成功了，那么这条指令操作的数据就永远不会丢失。**这就是我们的目的。

具体实现的方法我也为大家讲解清楚了：

**1 首先在主节点把接收到的每一条客户端指令包装成索引递增的日志，由主节点传输给集群中所有从节点。**

**2 通过主节点向从节点传输日志的方式，集群中的每个节点完成了数据备份和持久化，并且保证集群中各个节点状态一致。**

**3 集群中所有从节点需要对接收到的日志达成共识，也就是本地持久化成功，才能向主节点回复成功响应。**

**4 为了提升从节点响应主节点的效率，我们引入了日志复制冗余机制，规定在构建集群的时候，集群中节点个数为基数，必须符合 2n+1 个，最多允许出现故障的节点个数为 n 个，集群中只要有 n+1 个节点成功持久化了日志，就算是对这条日志达成了共识。主节点就可以应用这条日志包装的指令了。**

**5 当一条日志被集群应用了，主节点回复了客户端成功响应之后突然宕机，第 4 条得以实现的关键步骤就是，选举新的主节点时，必须选择拥有日志最多的从节点为主节点。这样，未完全完成复制的日志仍然可以复制给集群中的其他节点，从而使集群中的节点达到状态一致，集群可以正常提供服务。**

如果大家理解了上面的五条要点，最后我再来为大家梳理一下目前程序完整的执行流程，具体流程请看下面代码块。

```
1.客户端向集群主节点发送指令
2.主节点把指令包装成索引递增的日志
3.主节点把日志复制给集群中的每一个从节点
4.集群中 n+1 个节点持久化日志成功后，都向主节点回复了成功响应
5.主节点应用日志，然后向客户端回复指令应用成功的响应
```

以上就是目前程序完整的执行流程。看到这里也许有朋友会感到困惑，我一直在强调主节点应用日志，可是从来没有提到从节点该怎么应用日志。接下来我应该为大家简单解释一下：我们目前希望的就是集群中所有节点先对某一条日志达成共识，然后主节点应用日志，接着回复客户端成功响应。这时候集群中的从节点都还没有应用日志呢，但是日志已经达成共识了，这就意味着只要故障节点在允许的最大限制之内，已经应用的日志肯定就不会丢失了。反正从节点都会拥有这条日志，拥有了日志直接执行其中的指令不就行了吗？意思是这个意思，但是总得有一个通知呀，**从节点应用日志的前提是知道集群节点对这条日志已经达成了共识，但这个共识是主节点感知到的，然后主节点应用了这条日志。所以，集群中的从节点要应用这条日志，肯定是需要主节点来告诉从从节点，有一条日志已经被我应用了，你们也赶快执行吧。这时候，从节点就可以应用这条日志了。**

**这就意味着，每当集群中对一条日志达成共识后，主节点应用了这条日志，再主节点传输下一条日志给从节点时，需要把上一条已经应用的日志的索引传输给从节点，从节点接收到上一条被应用的日志的索引后，就可以根据索引应用自己本地的日志了。**

这时候，目前程序完整的执行流程就变成了下面这样，请看下面代码块。

```
1.客户端向集群主节点发送指令
2.主节点把指令包装成索引递增的日志
3.主节点把日志复制给集群中的每一个从节点
4.集群中 n+1 个节点持久化日志成功后，都向主节点回复了成功响应
5.主节点应用日志，然后向客户端回复指令应用成功的响应
6.客户端向主节点发送下一条指令
7.主节点接收到下一条指令后，把指令包装成日志，日志索引经过自增了
8。主节点在把当前日志发送给从节点的时候，也会把上一条被应用了的日志的索引发送给所有从节点
9.从节点接收到日志和索引之后，会将日志本地持久化，然后根据索引应用对应的日志
```

我能想到，也许有朋友还是感到十分困惑，目前这个流程理解起来确实没什么困难。但是，总觉得有些不对劲，因为看来看去，好像从节点要想执行已经被主节点应用的日志，只能让主节点进行通知才行。如果主节点不进行通知，那么从节点就无法执行已经被主节点应用了的日志。但是，尴尬的事情来了，主节点只有在接收到客户端指令的时候，把这条指令包装成日志的时候，才会向从节点发送消息，这时候才可以通知从节点应用已经被提交了的日志。假如客户端在发送了一条指令给主节点后，之后再也没有发送新的指令给主节点，那主节点就没办法第二次跟从节点传输消息了，也就没办法把已经应用的日志索引传输给从节点，这样一来，从节点不就一直无法执行已经被应用的日志吗？这怎么能使集群的各个节点达到状态一致呢？

要解决这个问题，就要引入一个新的知识模块了，那就是主节点和从节点之间需要时刻保持心跳连接，只有通过心跳连接，主节点才知道哪些从节点正常工作，哪些从节点发生了故障无法联系；也只有通过心跳连接，从节点才能知道主节点是否还在正常工作，主节点发生故障后，从节点可以立刻知道。发送心跳信息的时间可以由用户自己来配置，比如每过 1 毫秒，主节点就向从节点发送一个心跳信息。**如果主节点发送了心跳信息之后，在规定时间内并没有收到个别从节点回复的响应，这就意味着从节点可能发生故障了，可能是宕机或者是发生了网络分区；如果从节点在规定的时间内，并没有收到来自主节点的心跳信息，这就意味着主节点可能宕机或者发生网络分区了。那这个心跳信息和从节点执行已被提交的日志有什么关系呢？关系很明确，那就是主节点并不是只通过每一次复制日志给从节点的时候发送已被提交的日志索引，在每一次心跳信息中，也可以发送已提交的日志索引，而心跳信息发送是非常频繁的。这样一来，从节点就可以从主节点发送的心跳信息中获得已经提交的日志的索引，就可以应用对应的日志了。**

那么这一切该怎么实现呢？这就要从主节点选举机制说起了。这一章显然是无法讲完了，就留到下一章讲解吧。好了各位，我们下一章见。