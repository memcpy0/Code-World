大家好，在上一章的结尾我为大家引入了主节点和从节点的心跳连接操作，并且进一步为大家引出了主节点的选举操作。突然引出这个操作可能会让大家有些意想不到，本来我一直在为大家讲解主节点向从节点复制日志的知识，怎么好端端地又讲起来主节点选举机制呢？ 原因其实也很简单，就是想简单引入一下分布式的概念。请大家想一想，我从第一章就开始运用简单的主从复制机制，其实就是一个单主复制，来实现我程序中的数据备份，说到底是为了什么？还不是为了我的程序能正常运转，正常为用户提供服务。后来在实现单主复制地过程中，为了确保已经提交过的指令对应的数据不会丢失，又提出了诸多需要遵循的操作。**比如要让集群中的各个节点对一条日志先达成共识，然后才能提交。又或者当主节点宕机或者发生网络故障，选择新的主节点时，新的主节点需要拥有最新的日志，或者说主节点拥有的日志比其他节点新，至少也应该一致。**可以看得出来，什么主节点选举，主从复制，数据备份等等操作，其实都是分布式系统中非常流行的概念(当然，并不是说这些概念只应用于分布式系统中)。

而分布式系统是什么呢？将单体服务拆分成多个微服务，然后许多微服务同意对用户提供服务，这算是分布式系统的一种表现吗？当然算，这样一来处理用户操作会更快，更高效；单个服务器存放数据的容量达到极限，在应对高并发请求时处理数据效率低下，这时候将数据进行分片或者分区，这样不就可以提高程序性能了吗？这也是分布式系统的表现；将分片和分布区后的数据进行主从复制，增加数据备份，这仍然是属于分布式系统中的操作。所以，我就可以这样认为，我们构建的庞大的分布式系统，就是为了是我们的程序更加健壮，安全，性能更高，容错性更高。

**多主复制的可行性**

好了，现在就让我们看看这个主节点选举吧。一个团体如果没有一个领头人发号施令，可能整个团体就会像一盘散沙。当然，如果这个领头人是个垃圾，可能整个团体连一盘散沙都不如。集群中也是这样，一个集群中没有主节点，那么从节点根本就不知道要干什么了。当然，可能有人会说，这些都是程序员自己定义的。就比如说，在我们目前的程序中，我自己做了一个规定，**只有主节点能够和客户端打交道，其他的从节点虽然都部署着和主节点相同的程序，都可以计算营业总额，也可以记录私人物品存放位置，但是，这些从节点不能直接与客户端打交道。要想数据和主节点保持一致，就只能通过应用从主节点复制过来的包装了客户端指令的日志，实现主从节点的数据和状态统一。当然，从节点并不是完全不能和客户端打交道，严谨地说，集群中的主节点可以处理客户端地写操作指令，而从节点只能负责处理客户端的读操作指令。**

那如果改变一下规定，允许从节点和客户端打交道呢？也就是允许从节点处理客户端的写操作请求，就比如说现在有一个集群，这个集群有 3 个节点，其中节点 1 是主节点，可以处理客户端的写操作请求，节点 2 和节点 3 是从节点。虽然是从节点，但是我允许集群中的节点 2 也可以处理客户端的写操作。这么着一来，实际上就相当于整个集群中有 2 个主节点来处理客户端的写操作请求。

大家可以想象一个场景，就比如说我在第一章为自己程序构建的集群，本来集群中只有一个主节点，两个从节点，但是，有一天我老婆忽然试探我，问我是不是在私藏小金库。我说我没有私藏小金库，但我老婆接着说，宝贝，你为什么让自己这么累呢？每天工作已经很累了，还要小心翼翼地藏私房钱，你想买什么直接告诉我不行吗？我又不是不给你买。女人的这种话骗骗小年轻还行，不好意思，我已经是三十多岁的人了，分得清真话假话。所以，我仍然说我没有私藏小金库。但紧接着我媳妇又说，好了宝贝，我知道你一直在偷偷攒钱，偷偷攒钱一定很辛苦吧，你想买什么就直接跟我说吧，我今天玩手机时看见我们上大学时的照片了，现在你憔悴了很多，我真很心疼你，我们俩之间应该存在秘密吗？如果你非要偷偷攒钱，就攒吧，我想跟你一起攒钱，每个月我会给你 6000 块，这样你攒起来更快。你要是没攒钱就算了，反正钱在我这里，我也不会乱花，你想买什么直接告诉我就行，我给你买。你是我的宝贝，应该学会爱自己，更值得被爱。很抱歉，哥们功力不深，还是被媳妇的话整破功了，三言两语交代了自己私藏小金库的事。结果可想而知，我的小金库被媳妇席卷一空了。不仅如此，我的服务器地址也被媳妇知道了，这样一来，我每攒一笔钱，媳妇就都知道了。

现在的情况是，我会经常访问集群中的主节点 1，媳妇经常访问集群中的从节点 2，这两个节点都可以处理客户端的写请求。虽然我把节点 2 成为从节点，但它已经具备了主节点的功能，实际上就相当于整个集群中有了 2 个主节点，也会有 2 个客户端来访问这两个节点。 就这样，我还是继续存着钱，当然，每挣一笔钱都会立刻被媳妇搜刮走。很快，我就发现这么做给我的集群造成了一个严总问题，那就是日志根本无法在两个主节点之间复制。

比如说吧，在节点 1 和节点 2 的 map 成员变量中，都有这样一条记录，请看下面代码块。

```
key：挣到的 1000 块钱，value：还没有交给老婆。
```

某一个时刻，我挣到了 1000 元，访问了主节点 1，将这条记录更改为了下面这样。

```
key：挣到的 1000 块钱，value：已经交给老婆了。
```

如果节点 1 中的日志索引已经自增到 10 了，这样一来，节点 1 将会产生一条索引为 11 的日志，然后将这条日志赋值给集群中的另外两个节点，也就是节点 2 和节点 3。其中节点 2 也可以处理客户端写操作。

与此同时呢，我老婆访问了节点 2，将这条记录更改为了下面这样。

```
key：挣到的 1000 块钱，value：怎么还没有把钱上交啊。
```

这时候，节点 2 也会在本地产生一条日志，日志的索引也自增到 10 了，这样一来，节点 2 和节点 1 产生的日志根本无法成功复制给对方。因为本地日志的索引都是 10，这就意味着要接收的下一条日志的索引肯定为 11，但传输过来的确是 10，所以不会接收。由此可以推断，如果在我目前的集群中定义两个主节点，也就是多主复制，很容易出现日志索引冲突的问题。

当然，这时候也许会有人建议我在集群中引入一个全局的日志索引分发器，首先我肯定不会建议这么做。但是，我们可以先试着分析一下，假如引入了这个日志索引分发器会怎样。显然，这时候节点 1 产生的日志索引就为 10，而节点 2 产生的日志索引就为 11。这样节点 1 和节点 2 的日志就会互相复制给对方了。但是这样一来，请大家想一想，如果节点 1 刚刚把 map 中的记录修改为下面这样。

```
key：挣到的 1000 块钱，value：已经交给老婆了。
```

在收到节点 2 的日志后，可能会在条件满足的情况下应用这条日志，于是节点 1 中 map 的记录就变成了下面这样。

```
key：挣到的 1000 块钱，value：怎么还没有把钱上交啊。
```

我他妈不是刚刚把钱给了老婆吗？怎么又特么没有上交了？

节点 2 也会在条件满足的情况下应用节点 1 发送过来的日志，这样一来，节点 2 中 map 的记录就会变成了下面这样。

```
key：挣到的 1000 块钱，value：已经交给老婆了。
```

显然，集群中已经有两个节点的数据状态不一致了，这是我不希望看到的。这时候，再让我来给大家阐述一下我自己的观点，为什么不引入这个全局的日志索引分发器。**因为我为一开始单机部署的程序构建了一个集群，本意就是为了使程序对外的容错性更高，这个容错性可以包含很多方面，比如提供服务的主节点故障了，很快就能选择一个从节点承担主节点的职责，继续工作，数据多了好几重备份，也变得更安全了。说到底，我们搞了一大堆东西，把系统迭代或者重构得越来越复杂，也就一个目的，让程序更快，更安全。**

但现在要是引入一个全局的日志索引生成器，这不就又回到一个典型的单点故障系统就崩溃的局面了吗？一旦索引生成器出问题了，系统中的节点就不能正常工作了。容错性会大大降低。

由此可见，**集群中出现多个主节点，或者多个节点都可以处理客户端写请求是行不通的，当然，并不是没有相应的处理手段，因为除了单主复制，还有多主复制，甚至是无主复制的模式，**但是我不想搞得那么复杂。所以，在第二章，我总是会在某个情境中，假设主节点出故障了，要从剩下的从节点中选择一个新的主节点，并且只选择一个。这也就是说，在一个分布式系统中，你构建的某个集群，其中的主节点很可能是会变动的。就像我说我是家里的一家之主，是家庭的领导者，但是，只有干活的时候，刷锅刷碗的时候，媳妇才承认我是一家之主，因为一家之主要干更多的活；在管钱的时候，媳妇就成了一家之主，因为只有一家之主才有权力管理钱财。至于集群中主节点什么时候变动，就要看什么时候发生网络分区，或者什么时候主节点出故障了。接下来，我们就看看，当集群中的主节点出故障，要选取新的主节点时，需要遵循什么规则。

**主节点的选举过程**

其实在上一章已经为大家总结出一个要点了，在选举新的主节点时，**新的主节点需要拥有最新的日志，或者说主节点拥有的日志比其他节点新，至少也应该一致，**这一点就不再重复了。接下来，我们先思考一些小问题，然后再继续分析该如何选举新的主节点。请大家想一想，在进行新的主节点选举时，肯定意味着旧的主节点出故障了，那么，怎么就知道旧的主节点出故障了？在上一章我一直是使用手动的方式来选择新的主节点，并没有为大家分析从节点怎么感知主节点是否宕机。但现在不一样了，我们的程序逐渐步入正轨，肯定不能再使用手动选择的方式来选举主节点了。**正确的做法应该是，当主节点宕机或者发生网络分区时，从节点可以感知到主节点出故障，然后从节点们自发地开始进行选举，从剩下的从节点中选举出一个新的主节点。**所以，当前我们应该分析的是，集群中的从节点如何感知主节点出故障了。

这个问题其实上一章结尾也简单分析过了，那就是在主节点和各个从节点之间引入心跳连接，**确切地说，就是一个心跳请求。这个心跳请求是一个定时任务，主节点需要定时执行这个定时任务，在这个定时任务中，主节点会向每一个从节点发送心跳信息。从节点在收到来自主节点的心跳请求后，会向主节点发送一个成功响应。如果从节点在规定的时间内，没有收到来自主节点的心跳请求，从节点就会认为主节点可能出故障了，如果主节点发送了心跳请求给从节点，但是一直没有收到成功响应，那么主节点就会认为从节点可能出故障了。定义成定时任务，就可在规定的时间内发送，也就是每隔一段时间就发送一次心跳消息，只要心跳消息的发送和响应一直没有间断，就意味着集群中主节点和从节点一直保持着联系，可以正常提供服务。**说起来好像非常简单，所以接下来，我就从代码层面为大家实现一下。这样来看，我们的代码又要进行一次重构了。

首先是心跳请求本身，既然主节点要发送给从节点一个心跳请求，那就让我们想想，这个心跳请求本身该怎么定义。按照上一章的结尾，**为了能让从节点知道可以应用哪一条日志了，所以，可以在心跳请求中把要应用的日志的索引发送给从节点。**比如说我就把这个心跳请求定义为 HeratbeatRequest，具体实现如下。请看下面代码块。

```
public class HeratbeatRequest {

    //主节点应用的最新的日志索引，传递给从节点，也就是从节点将要应用的日志索引
    private Long committedIndex;

    //当前节点的地址
    private Endpoint endpoint;

    //主节点的ip地址
    private Endpoint master;

    public HeratbeatRequest() {
    }

    public HeratbeatRequest(Long committedIndex, Endpoint endpoint, Endpoint master) {
        this.committedIndex = committedIndex;
        this.endpoint = endpoint;
        this.master = master;
    }

    public Long getCommittedIndex() {
        return committedIndex;
    }

    public HeratbeatRequest setCommittedIndex(Long committedIndex) {
        this.committedIndex = committedIndex;
        return this;
    }

    public Endpoint getEndpoint() {
        return endpoint;
    }

    public HeratbeatRequest setEndpoint(Endpoint endpoint) {
        this.endpoint = endpoint;
        return this;
    }

    public Endpoint getMaster() {
        return master;
    }

    public HeratbeatRequest setMaster(Endpoint master) {
        this.master = master;
        return this;
    }
    
}
```

可以看到，我在上面的代码块中，不仅仅定义了一个 committedIndex 成员变量，还定义了 endpoint 和 master 这两个成员变量。这样一来，心跳请求就包含了从节点要应用的日志索引，也包含了要发送的从节点的 ip 地址，还包含了集群中主节点的 ip 信息，**这样一来当从节点收到心跳信息后，可以和自己本地记录的主节点的信息做对比。如果一致说明集群中主节点没有发生过变更，如果不一致，就意味着可能发生了网络分区，新的主节点把心跳信息发送过来了。**这个细节就不再本章展开讲解了，因为第四到第六章就会讲到了。

好了，现在心跳请求已经定义好了，是时候该重构一下 NodeImpl 这个类了。因为心跳请求是一个定时任务，需要在某个时刻启动，我们就不妨直接一点，让这个心跳定时任务在 NodeImpl 对象启动的时候就启动。启动了之后呢？这个心跳任务什么时候执行呢？肯定是设置一个心跳时间，比如说，在 1000 毫秒之后，主节点向从节点发送了一个心跳请求。接下来呢？什么时候发送第二次心跳请求呢？**也许有的朋友会想当然的以为心跳任务肯定是一个周期任务，所以就给这个定时任务设置一个执行周期，比如每 1000 毫秒执行一次，发送了第一个心跳请求之后，再过 1000 毫秒，就发送第二个心跳请求给集群中的所有从节点。**对此，我只能说这是一种比较常规的思路，但是，请大家再仔细想想，我们这样做就陷入一种局面，那就是为了发送心跳请求而发送心跳请求。别忘了，我们设计心跳请求的初衷是为了感知集群中主节点或者从节点是否有故障了。如果在某一次心跳请求中，发现某个节点出故障了，应该及时进行一些操作或者处理，而不是一直发送心跳请求给从节点。**所以，这就意味着，除了第一次向从节点发送心跳请求，之后的每一次心跳请求，主节点必须等待接收到从节点的心跳响应之后，才能继续发送。**如果心跳响应失败了，或者从节点返回的心跳响应中，告诉了当前主节点，从节点记录的主节点 ip 地址和当前主节点并不一致，这时候，主节点就要进一步处理了。心跳响应失败也很正常啊，对吧，就像一个人对你竖起大拇哥，可能这个人是在称赞你，也可能这个人正在用迫击炮瞄准你。所以，还需要对心跳响应做进一步判断，这个知识就留到后面再展开讲解吧。接下来，我们还是先重构一下 NodeImpl 这个类。

当然，在重构 NodeImpl 类之前，肯定要先实现这个发送心跳请求的定时任务器。因为是个定时任务，索性就直接使用 JDK 原生的 ScheduledThreadPoolExecutor 类吧。我就把这个心跳任务定时器定义为 TimerManager 吧。请看下面代码块。

首先是调度器 Scheduler 接口。

```
public interface Scheduler {

    ScheduledFuture<?> schedule(final Runnable command, final long delay, final TimeUnit unit);

    ScheduledFuture<?> scheduleAtFixedRate(final Runnable command, final long initialDelay, final long period,
                                           final TimeUnit unit);

    ScheduledFuture<?> scheduleWithFixedDelay(final Runnable command, final long initialDelay, final long delay,
                                              final TimeUnit unit);
    void shutdown();
}
```

然后是 Scheduler 的实现类 TimerManager。

```
//心跳任务定时调度器
public class TimerManager implements Scheduler {

    //真正的定时任务执行器，使用的就是jdk的ScheduledThreadPoolExecutor
    private final ScheduledExecutorService executor;

    public TimerManager(int workerNum) {
        this(workerNum, "JRaft-Node-ScheduleThreadPool");
    }

    public TimerManager(int workerNum, String name) {
        //通过线程池工具类创建了一个定时任务调度器
        //核心线程数就定义为20个吧
        this.executor = Executors.newScheduledThreadPool(20);
    }

    //下面几个都是提交定时任务的方法
    @Override
    public ScheduledFuture<?> schedule(final Runnable command, final long delay, final TimeUnit unit) {
        return this.executor.schedule(command, delay, unit);
    }

    @Override
    public ScheduledFuture<?> scheduleAtFixedRate(final Runnable command, final long initialDelay, final long period,
                                                  final TimeUnit unit) {
        return this.executor.scheduleAtFixedRate(command, initialDelay, period, unit);
    }

    @Override
    public ScheduledFuture<?> scheduleWithFixedDelay(final Runnable command, final long initialDelay, final long delay,
                                                     final TimeUnit unit) {
        return this.executor.scheduleWithFixedDelay(command, initialDelay, delay, unit);
    }

    @Override
    public void shutdown() {
        this.executor.shutdownNow();
    }
}
```

接下来就是 NodeImpl 类的，请看下面代码块。

```
//节点实现类
public class NodeImpl{

    //这个成员变量记录的就是我一共挣到了多少钱
    private Integer totalMoney;

    //记录每一个私人物品藏在哪个位置了
    private Map<String,String> map = new HashMap<>();

    //当前节点的状态，是主节点还是从节点
    private State state;

    //记录集群中所有节点IP地址的成员变量
    List<Endpoint> list = new ArrayList<>();

    //记录集群中主节点的IP地址
    Endpoint master;

    //当前节点自己的IP地址
    Endpoint ip;
    //最后发送或者是接收到的指令的索引
    private long lastIndex;

    //心跳定时任务调度器
    private Scheduler timerManager = new TimerManager();

    //心跳任务延时发送时间
    private int heartbeatDelayMs = 1000;

    //主节点应用到哪条日志了，记录日志的索引
    private long appliedIndex;


    public NodeImpl() {

    }

    //构造方法
    public NodeImpl(State state, List<Endpoint> list, Endpoint ip) {
        this.state = state;
        this.list = list;
        this.ip = ip;
    }

    //其他方法省略

    //假装是启动节点服务器的方法
    public void start() {
        System.out.println("启动当前节点服务器，当前节点可以收发消息了！");
    }

    //启动心跳定时器的方法，所谓启动，也就是向心跳定时器中提交发送心跳请求的定时任务
    private void startHeartbeatTimer() {
        //向定时任务调度器中提交一个发送心跳消息的定时任务，这个任务会在1000毫秒之后执行
        this.timerManager.schedule(() -> sendHeartbeat(), 1000,TimeUnit.MILLISECONDS);
    }

    //真正发送心跳请求给从节点的方法
    private void sendHeartbeat() {
        for (int i = 0; i <list.size() ; i++) {
            Endpoint endpoint = list.get(i);
            if (endpoint.equals(ip)) {
                //如果当前遍历到的节点信息和当前节点信息相等，就直接跳过本次循环
                //不可能自己给自己发送消息
                continue;
            }
            HeratbeatRequest heratbeatRequest = new HeratbeatRequest();
            //设置心跳请求要发送给的从节点的ip地址，设置主节点信息，设置主节点应用的
            //最新日志的索引
            heratbeatRequest.setEndpoint(endpoint).
            setMaster(master).
            setCommittedIndex(appliedIndex);
            //交给rpc的线程将心跳请求发送给从节点，这里就写成伪代码吧
            System.out.println("将心跳请求发送给从节点！");
        }
    }


    //从节点接收到心跳请求后，处理心跳请求的方法
    public HeratbeatResponse handleHeratbeatRequest(HeratbeatRequest request) {
        //首先从心跳请求中获得主节点信息
       Endpoint ip = request.getEndpoint();
        //判断心跳请求中的主节点是否和本地记录的主节点是否相同
        if(!ip.equalsmaster()){
            //如果不相等，返回失败响应给主节点
            //直接写成伪代码了，这里其实是可以把从节点记录的主节点封装到响应体中
            //返回给主节点，但我这里就不这么做了，因为这不是真正的做法，就没必要实现了
            return FailResponse
        }

        //走到这里，意味着主节点信息没问题，直接让从节点应用日志即可，也写成了伪代码
        Long index = request.getCommittedIndex();
        System.out.println("从节点根据日志索引应用日志");
        //返回成功相应给主节点
        return SucResponse;
    } 


    //主节点接收到从节点的心跳响应后，处理心跳响应的方法
	 public void onHeartbeatReturned(HeratbeatResponse response) {
         //先判断响应状态是否为成功
         if(!response.getSuccess()){
             //走到这里意味着响应不成功，可能是主节点信息不一致
             System.out.println("从节点返回失败响应！");
         }

         //走到这里意味着响应没问题，那就可以提交下一个心跳定时任务给心跳任务定时器了
         //调用下面这个方法，在1000毫秒之后会发送下一个心跳请求给从节点
         startHeartbeatTimer();
     }


    
}
```

这就是我重构好的 NodeImpl 类，当然，类中删除了很多方法，只保留了和处理心跳请求有关的方法。简单来说，**就是主节点会调用 sendHeartbeat 方法给每一个从节点发送心跳消息，从节点会通过 handleHeratbeatRequest 方法处理主节点发送过来的心跳请求，然后给主节点发送一个心跳响应，然后主节点会通过 onHeartbeatReturned 方法处理从节点发送过来的心跳响应。**逻辑非常简单，就不再详细讲解了，因为接下来，还有一个新的问题需要我们来分析，**第一次心跳消息什么时候发送呢？答案很直接，肯定是主节点被选举出来之后，就要向从节点发送心跳消息。**那么主节点什么时候才会被选举出来呢？这是我们接下来要真正讨论的问题。这个问题分为两种情况。

第一种情况：**集群中已经有主节点了，主节点一直在向各个从节点发送心跳消息，持续了一段时间之后，主节点可能出现故障了，就这样，心跳连接断了。在有一段时间没有接收到来自主节点的心跳消息后，集群中的从节点开始自发地进行主节点选举了。如果选举出一个新的主节点，那么新当选的主节点就要开始向各个从节点发送心跳消息，然后就是按照我们上面代码块展示的那样，在收到心跳回应后继续发送心跳消息。**

第二种情况：**整个集群刚刚启动，还没有正式开始工作的时候，这时候肯定也应该先从集群中的各个节点中选出一个主节点，然后主节点才能开始处理客户端的写指令。**

以上就是两种需要进行主节点选举的情况，虽然我列举了两种情况，但是，这两种情况都可以用一句话来概括：**那就是当从节点有一段时间没有接收到来自主节点的心跳请求后，就要自发地进行主节点选举。**这句话肯定适用于第一种情况，那么第二种情况该怎么理解呢？这个也很简单，当集群刚刚启动的时候，集群中所有节点都是从节点，这时候还没有主节点，那所有从节点肯定都接收不到心跳请求，超过一段时间之后，就要开始自发地进行主节点选举了。这个应该也很容易理解吧？

**所以，现在问题的重点就变成了，在从节点多久没有接收到来自主节点的心跳请求后，才要开始进行主节点选举呢？**理清楚这个问题非常重要，因为要考虑到主节点给从节点发送消息的时间间隔，**假如说主节点每过 1000 毫秒向从节点发送一次心跳消息，但是从节点 500 毫秒没有收到来自主节点的心跳消息，就会自发地进行主节点选举，这显然是不合适的，也就是说，主节点向从节点发送心跳消息的时间间隔，必须小于从节点没有收到主节点心跳请求开始自发选举主节点的时间间隔。也就是说，主节点发送心跳消息的间隔时间，必须小于从节点没有接收到心跳消息从而开始选举主节点的时间间隔。**现在我就把从节点没有接收到心跳消息从而开始选举主节点的时间间隔称为超时选举时间。好了，现在我就再次定义一下，我把集群内部主节点发送心跳的心跳时间间隔定义为 1000 毫秒，把从节点超市选举时间定义为 1500 毫秒。这样一来，就能保证，集群中的从节点确实是在没有接收到心跳消息后，才开始进行新的主节点选举的。

**解决多个主节点同时选举问题**

很好，这个问题就算讲解完毕了，下面就开始真正的主节点选举了。这时候，我们又遇到了另外一个问题，那就是主节点要以什么样的方式进行选举，我的设计思路和之前日志复制时的一样，那就是让被选举出来的主节点在集群中达成共识。具体实现也很简单，**那就是当集群中的从节点开始自发进行新的主节点选举时，从节点会向其他从节点发送一条请求消息，在这条请求消息中封装当前从节点本地最后一条日志的索引，发送给其他从节点之后，其他从节点会从这条请求选举的消息中获得发送消息过来的从节点的最后一条日志的索引，如果这个索引比自己本地最大的日志索引还要大或者相等，那么接收到消息的从节点就会给发送消息的从节点回复一个成功响应，如果请求中的日志索引还没有本地最大的日志索引大，那接收到消息的从节点直接忽略这个请求即可。这样一来，只要集群中某个节点接收到超过集群一半节点的成功响应后就可以成功当选新的主节点了。**

可能这时候有朋友会有些怀疑，我这种设计方式**，真的可以让集群中拥有最新日志的节点当选为新的主节点吗？并且还能保证新的主节点的最新的日志一定是被成功应用了的？**接下来，让我来进一步为大家分析一下。其实现在我们面临两个问题，第一个：集群中各个节点日志复制的进度可能并不是一致的，因为网络会有波动，旧的主节点一直在接收客户端指令，然后不停地把指令包装成索引自增的日志，一直发送给从节点。但是每一个从节点所处的网路环境可能是不同的，比如集群中一共有 5 个节点，节点 1 为主节点，其他都为从节点。可能主节点 1 一共生成了 10 条日志，然后复制给所有从节点，但是集群中的从节点们网络环境不一样，有的收到了完整的日志，有的还没有复制完整，比如 从节点 2、3 都完整复制了 10 条日志，而从节点 4 只复制了 8 条日志，从节点 5 只赋值了 7 条日志。这样一来，当旧的主节点出现故障，经过 1500 毫秒之后，集群中的从节点仍然没有收到来自主节点的心跳请求，都触发了超时选举。于是各个从节点都会向其他所有从节点发送请求选举的消息。也就是说一瞬间集群中的所有从节点都会触发超时选举，可想而知，局面会特别混乱。我希望的是集群中每次选举新的主节点时，只有一个节点可以触发超时选举，如果该节点成功当选，那就直接可以结束主节点选举了，而不是大家一窝蜂地都来选举。所以，这个问题急需我们解决，那就是避免集群中出现"超时选举风暴"；第二个问题就很清晰了，那就是怎么保证新当选地主节点，一定拥有最新的已经被应用了的日志。

接下来就让我们先解决第一个问题，既然要避免集群中出现选举风暴，那我直接把各个从节点的超时选举时间错开不就行了？**以前大家都是在 1500 毫秒没有接收到来自主节点的心跳请求后开始进行主节点选举，现在我给每一个节点的超时选举时间加上一点随机数，这样一来，每一个节点的超市选举时间就不相同了，但是又都是大于 1500 毫秒。**就比如说，我把集群中的超市选举时间定义成下面这样。请看下面代码块。

```
 int electionTimeoutMs = ThreadLocalRandom.current().nextInt(1500, 2500);
```

通过上面代码块的调整，每个从节点的超市选举时间就都是随机的了，范围在 1500 - 2500 毫秒之间，也就是说，有的节点可能在 1500 毫秒没有接收到来自主节点的心跳消息后，就开始进入主节点选举阶段了，而有的节点可能在 2000 毫秒之后才进入超时选举节点，有的节点则在 2300 毫秒之后，**这样将各个节点的超时选举时间区分开，可能在第二个从节点还没有触发超时选举时，第一个触发超时选举的从节点就已经成功当选主节点了，然后发送了心跳消息给其他从节点，其他从节点也就不会再进入超时选举阶段，集群又开始正常提供服务了。**

虽然我们通过随机数避免了集群选举风暴，可就是有一个谁也想不到的巧合，集群中两个从节点的随机超时选举时间相同了，这时候会发生什么样的情况呢？假如还是上面那个例子，集群中有 5 个节点，节点 1 为主节点，宕机了，节点 2、3 都已经复制了 10 条日志，这两个节点的超时选举时间相同了，于是都进入了超时选举阶段，情况就变成了下面这样。

![](https://cdn.nlark.com/yuque/0/2024/jpeg/26725125/1704341085275-4a98019d-d895-4c9c-ac06-4ccc6dbb1bd5.jpeg)

节点 2 和节点 3 都会向节点 4、5 发送选举请求.还会向彼此发送选举请求，之前我们说过，**当某个从节点接收到的选举请求响应数量超过集群一般节点数量时**，就可以认为这个从节点成功当选了主节点，现在节点 2 和节点 3 都进入了选举阶段，自己选举当然会首先同意自己当选，所以节点 2、3 会首先将各自接收到的请求响应数量加 1。而节点 4 和节点 5 发现节点 2、3 的最后一条日志的索引都比自己的大，所以节点 4、5 都会给节点 2、3 回复成功响应，节点 2 发现节点 3 的最后一条日志索引和自己一样，节点 3 也发现节点 2 的最后一条日志索引和自己一样，也会给对方回复成功响应。这样一来，节点 2、3 接收到的响应数量就都是 4。响应数量都一样，也都超过了集群中过半节点的数量，那谁来当主节点呢？节点 2、3 可能都会认为自己是主节点，于是集群中就出现了 2 个主节点，这显然不是我们愿意看到的情况。这下又该怎么解决呢？没办法了，只能再添加一个限制了，**那就是集群中的从节点回复选举请求的成功响应时，只能回复一次。这就意味着一个从节点一旦给某一个发送了投票请求的从节点回复了成功响应，就不能再向另外的节点回复成功响应了，哪怕另外的从节点拥有的最后一条日志索引也比自己的大。**

这样一来，假如节点 2 的请求先于节点 3 的请亲到达节点 4 和节点 5，那么节点 4 和节点 5 就只会给节点 2 回复成功响应，就不会再回复节点 3 了。而节点 2 已经选自己为主节点，也就不会再回复节点 3 了。节点 3 选择了自己当主节点，也就不会再回复节点 2 了。所以到最后，节点 2 一共收获了 3 个成功响应，节点 3 只有一个。这么一来，只有节点 2 收到的响应超过了集群中过半节点的数量，因此成功当选为新的主节点。很好，问题到这里就解决了。

不过，如果继续刨根问题，很快我们就会发现，添加了每个节点只能回复一次响应的限制后，还是会出现无法成功选举主节点的问题。比如说节点 2 的请求比节点 3 的请求先到达节点 4，节点 4 给节点 2 回复了成功响应；但是节点 3 的请求比节点 2 的请求先一步到达节点 5，这时候节点 5 就会给节点 3 回复成功响应，而不会给节点 2 回复成功响应。最后，加上自己选举自己，节点 2 和节点 3 收到的成功响应都为 2，集群中有 5 个节点，所以两个节点收到的成功响应都没有超过集群节点数的一半，也就没办法选举出新的主节点。

这时候该怎么办呢？要知道，上面这种情况并不是没有发生的可能，我们只能通过各种手段，尽量避免这种情况的发生。**比如，我们可以进一步增加超时选举时间的随机性，还可以进一步扩大超时选举时间的范围，当然，不能过大，否则集群就没办法正常工作了。**但还是发生了这种情况，这时候又该怎么办呢？这时候就只能重新进行选举了。**也就是说当一个从节点发送了选举请求给其他从节点之后，迟迟没有收到超过集群半数节点的成功响应，那么这个从节点就可以开启下一轮主节点选举了。也就是说，当已经参加了一轮选举的从节点再等待一个随即选举超时时间之后，仍然没有收到心跳信息，这说明集群中还是没有新的主节点产生，那就可以开启下一轮选举。已经选举过一轮的从节点也可以再次参选了，直到选举出新的主节点位置。**

但是这样一来，就又带来了一个问题，因为之前我们添加了一个限制，那就是在进行主节点选举的时候，每个从节点只能给一个发送了选举请求的从节点回复成功响应，但现在又开始了新一轮选举，明明刚才节点 4 已经给节点 2 回复了成功响应，但是你节点 2 自己没有当选成功，现在又来继续选举了，那节点 4 还回复吗？当然应该回复，否则节点 2 的第二轮选举不就没意义了吗？但是节点 4 又不知道节点 2 已经开启了第二轮选举。**所以，我们还需要在程序中为每个节点引入一个新的成员变量，记录当前节点参与选举的轮数，当然也应该添加另外一个成员变量，用来记录当前节点在当前轮数中为哪个节点投过票。**就像下面这样。

```
//节点实现类
public class NodeImpl{

    //节点参与选举的轮数，轮数的初始值为0
    private long term;

    //记录当前节点在当前轮数中为哪个节点投过票
	private Endpoint node;

    //省略其他成员变量  


    public NodeImpl() {

    }

    //构造方法
    public NodeImpl(State state, List<Endpoint> list, Endpoint ip) {
        this.state = state;
        this.list = list;
        this.ip = ip;
    }

    //其他方法省略
    
}
```

现在集群节点选举的情况就变成了这样：**集群中旧的主节点突然宕机了，在从节点 1 在经过自己的随机超时选举时间之后，发现仍然没有接收到来自主节点的心跳消息，于是进入了超时选举节点。在超时选举阶段，首先把自己的 term 成员变量自增 1，由 0 变为了 1，代表自己正在进行第一轮选举，然后把自己的 term 和最后一条日志的索引封装到选举请求中，把这个请求发送给各个从节点。从节点在接收到选举请求之后，首先用请求中的 term 和自己本地的做对比，如果两个 term 一样，就会接着去查看自己有没有给发送过来请求的从节点投过票，如果投过票，就会直接忽略这个投票请求，如果没有投过票，那就会接着去对比请求中的日志索引是否小于本地最新日志索引，如果小于就直接忽略这个请求，否则就返回成功响应。如果在判断 term 的时候，直接发现请求中的 term 比自己本地的大，说明这是新一轮的选举，这时候收到请求的从节点就需要把自己的本地的 term 更新一下， 用请求中的 term 为本地的 term 赋值，表明自己已经参加了新一轮的选举，然后就可以去判断日志索引，再决定是否应该在本轮选举中返回成功响应即可(其实这里应该把日志索引和term结合到一起，才能判断哪条日志是最新的，在 raft 算法中就是这么做的，但这个知识点我留到引入日志组件时再讲解，现在大家理解意思即可)。如果返回的是成功响应，那么从节点还需要记录自己为哪个节点返回了成功响应，这样当再接收到新的选举请求时，就算应该回复成功响应，也不必再回复了，因为在这一轮选举中，已经给一个节点回复了成功响应。**

这样一来，即便集群中真的出现了两个节点同时进行主节点选举的情况，就比如说是节点 2 和节点 3 吧，那么节点 2 和节点 3 都开始进行主节点选举时，都会将自己的轮数自增 1，所以节点 2 和 3 的轮数都变成了 1，然后分别向节点 4、5 发送投票请求，节点 4、5 就会按照刚才的分析为节点 2、3 回复响应。如果第一轮无法选举出新的主节点，还可以自增轮数，进行多轮选举，最终选举出一个新的主节点。

好了，现在主节点选举的知识也讲解得差不多了。这会儿再让我们回到构建集群的初衷，**就是希望程序的数据能够得到备份，已应用的指令产生的数据不会丢失，程序具备容错性，主节点发生故障后，立刻选举出新的主节点继续提供服务。**这里面我们最希望的还是已经应用了的指令产生的数据不会丢失，这个才是最重要的，那现在我们来看一看，如果在集群中使用上面我为大家设计的方式来选举主节点，这么做会不会让已经应用的指令丢失呢？也就是让已经应用了的日志丢失。

我们还是以上面的小例子来分析：集群中的主节点为节点 1，这个主节点产生了 10 条日志，分别复制给了其他 4 个从节点，当然，有的从节点日志复制得并不完整，节点 2 和节点 3 都复制完整了，这两个节点拥有 10 条日志，日志索引从 1 到 10，节点 4 复制了 8 条日志，日志索引为 1 到 8，节点 5 复制了 7 条日志，日志索引为 1 到 7。主节点 1 宕机之后，假如是节点 5 首先触发了超时选举，节点 5 就会将自己的 term 自增 1，然后把 term 和最后一条日志得索引封装到选举请求中，发送给其他的从节点。其他从节点接收到选举请求后，发现请求中的 term 比自己的本地的大，就知道主节点出故障了，现在触发了新的选举，于是都去判断请求中日志的索引是否比本地最新的日志索引小，结果节点 2、3 、4发现请求中的日志索引小于本地最新的日志索引，就不会返回成功响应，节点 5 虽然进入了选举阶段，但是只有自己认为自己可以当选，其他从节点都不同意，就相当于投票选举主节点，只有自己投了自己一票，没有超过集群半数节点，显然无法当选主节点。

假如过了一会儿，节点 2 也触发了超时选举，进入了选举节点，**注意，这时候节点 2 的 term 自增之后是 2，因为之前接收节点 5 请求的时候，已经把自己的 term 更新为 1 了。**它的最后一条日志的索引为 10，所以发送请求之后，因为是新的选举轮数，所以会收到所有其他从节点的成功响应(**这里有一个小细节，那就是节点 5 给节点 2 投票的问题，类比到 raft 算法中，其实就是候选者在什么情况下可以给另一个候选者投票的问题，这个问题下一章再为大家展开讲解**)，这样一来，节点 2 就成功当选为新的主节点了。而节点 2 拥有完整的日志，不管日志有没有应用，最终都可以复制到其他从节点上。

接下来，让我们再换另一种情况来分析，假如集群中节点 3 并没有复制完整 10 条日志，只复制了 9 条，并且在主节点宕机之前，日志也应用到了第 9 条。这时候，如果节点 2 触发了超时选举，一定可以被选举成功；如果是节点 3 触发了超时选举，除了节点 2 不会回复成功响应，节点 4、5 都会回复成功响应，自己还会投自己一票，所以节点 3 也会成功当选主节点。当然，如果节点 3 当选了主节点，节点 3 并没索引为 10 的日志，那么节点 2 索引为 10 的这条日志可能会丢失，或者会被主节点 3 新的索引为 10 的日志覆盖(这个后面引入日志模块之后再展开讲解)，这是可以接受的，因为日志 10 并没有应用到集群中。我们所做的一切都是为了使应用的日志不丢失，节点之间的数据一直，节点状态一致，做到这个就够了。

上面这两个小例子就充分地告诉我们一点：**主节点一定拥有全部已经应用了的日志，因为已经应用了的日志一定复制到集群中过半的节点中了。如果一个节点最新的日志索引小于集群中已经应用的日志索引，那这个节点一定不会成功当选主节点，因为它的日志索引一定比集群过半节点的日志索引要小，而当选领导者需要集群过半节点的同意。**就是这么个道理。

**总结**

到此为止，要分析的内容几乎都讲解完毕了。其实讲到这里也可以明说了，前三章讲解的其实就是 raft 共识算法的知识，从一开始的将客户端指令包装为日志传输给从节点，到第二章的日志复制过程，再到第三章的主节点选举。其实主节点选举在 raft 算法中叫做领导者选举，还给节点定义了跟随者，候选者，领导者的身份；文章中的选举轮数对应的也就是 raft 算法中的任期。这些都是 raft 算法的一些概念，但在我看来，概念只是概念，重要的是实现原理，实现原理一致，概念或者名词怎么解释都可以，只要能让人理解掌握就行。就像 raft 算法本身，说白了 raft 算法不就是单主复制的一种具体实现吗？单主复制是一个宽泛的概念，可以有不同的实现方式，raft 只是其中的一种。当然，我目前为大家讲解的 raft 算法的知识还不太全面，只是展示了一个大概的执行流程，其中还有很多细节没有铺展开讲解，比如集群中出现脑裂了该怎么办？新旧领导者交替时，日志发生冲突了该怎么办。日志传输过程中有遗漏该怎么办等等，这些细节我准备留在后面构建 sofajraft 框架时一点点剖析讲解。大家可能也看到了，前三章代码比较少，而且就这么点代码，还夹杂了很多伪代码。倒不是我懒得为大家写代码，而是我要是一点点写代码，定义各种请求体，响应体，再把 rpc 也实现了，这就太麻烦了。因为从第四章开始，我就会一点点地带领大家构建一个 raft 共识算法框架了，那时候会写代码，而且会写很多。前三章文字描述很多，虽然举了很多小例子，但还是有些啰嗦，如果在这种代码很少的情况下硬要去讲解更加细节的一些知识，只会让文章越来越长，还达不到清晰流畅的效果。所以，我愿意把很多细节知识放到后面讲解，在后面的章节中还可以配合代码一起来讲解。

现在，让我再来为大家梳理一下，整个程序的执行流程，也就是对我自己实现的 raft 共识算法做一次总结。

**1 首先在主节点把接收到的每一条客户端指令包装成索引递增的日志，由主节点传输给集群中所有从节点。**

**2 通过主节点向从节点传输日志的方式，集群中的每个节点完成了数据备份和持久化，并且保证集群中各个节点状态一致。**

**3 集群中所有从节点需要对接收到的日志达成共识，也就是本地持久化成功，才能向主节点回复成功响应。**

**4 为了提升从节点响应主节点的效率，引入了日志复制冗余机制，规定在构建集群的时候，集群中节点个数为基数，必须符合 2n+1 个，最多允许出现故障的节点个数为 n 个，集群中只要有 n+1 个节点成功持久化了日志，就算是对这条日志达成了共识。主节点就可以应用这条日志包装的指令了。**

**5 为了让集群主节点出现故障时，集群从节点可以自发地选举新的主节点，引入了心跳机制。主节点需要定时向从节点发送心跳请求，从节点在选举超时时间内没有收到心跳请求，就可以进入主节点选举阶段了。**

**6 为了避免集群中出现选举风暴的情况，为选举超时时间增加了随机性，但是这个选举超时时间必须是要大于心跳间隔时间的。这样一来，可以在很大程序上实现：每次只有一个从节点率先进入选举阶段。如果该节点被成功选举为主节点，就要向其他从节点发送心跳信息。这样一来，其他节点就不必进入选举节点了。**

**7 尽管引入了随即选举超时时间，但集群中还是有概率出现两个从节点同时进入主节点选举阶段的情况，为了使主节点选举成功，所以又引入了每个从节点只能回复一个成功响应的限制；随后又引入了轮数这个给概念，也就是 raft 共识算法中的任期概念。实施多重手段，最终能保证集群可以快速选举出新的主节点。**

**8 在从节点回复另一个从节点发送过来的选举请求时，如果请求中的日志索引比自己本地最新的索引小，则可以直接忽略这条选举请求；如果请求中的 term 也小于从节点的本地 term，也可以直接忽略这条请求。因为当前从节点可能已经在参与第 3 轮主节点选举了，结果因为网络波动，第 2 轮的选举请求才发送过来。当然可以直接忽视。**

**9 当一条日志被集群应用了，主节点回复了客户端成功响应之后突然宕机，选举新的主节点时，必须选取拥有所有已应用的日志的从节点为主节点。这样，未完全完成复制的日志仍然可以复制给集群中的其他节点，从而使集群中的节点达到状态一致，集群可以正常提供服务。**

**10 集群的从节点自发选举主节点时，第 9 条一定会成立，因为已经被应用了的日志一定复制到集群中过半的节点中了。如果一个节点最新的日志索引小于集群中已经应用的日志索引，那这个节点一定不会成功当选主节点，因为它的日志索引一定比集群过半节点的日志索引要小，而当选领导者需要集群过半节点的同意。**

以上就是前三章我为大家讲解的知识，其实都是 raft 算法中的内容。我没有直接讲解论文或者是按照某本书籍讲解，因为干讲理论十分枯燥。如果能通过一个个例子，让大家知道这样做行不通，转而去分析正确的实现方法，我想，这种方式会让大家对知识理解得更深。我已一直觉得，在学习时，理解知识比记住知识更重要。

在接下来的章节，我就会先用以上讲解的这几个要点，带领大家一点点构建一个工业级别的 raft 共识算法框架，当然，在开发的过程中会对现有的知识进行更多细致的优化，到时候大家就清楚了。各位，我们下一章见。