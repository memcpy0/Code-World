我特意从校内FTP下载了很多资料，包括RFC文档。其实当时并没有阅读它们的打算，只是想存在电脑里，以备不时之需。不过工作了几年后，我开始意识到RFC不只是用来检索的。**对于最棘手的那部分网络问题，有时必须熟读RFC才能解决，我手头就有很多例子可以证明这一点**。

老油条的工程师都知道，性能问题是最难的，因为没有任何报错可以入手，我们就来说一个性能相关的案例吧。有家公司跟我反映过这样一个问题，他们的客户端发数据到国外服务器时非常慢。慢到什么程度呢？连期望值的一半都达不到，就像你家里租了100M带宽，但实际用起来却不到5M的效果，肯定会不满意。

现实中这类问题往往是这样收场的：用户向运营商投诉带宽不足；运营商用测速工具自证清白；用户掏钱租用更多带宽。
其实用不着多花钱，用Wireshark仔细分析一下，基本都能找到提升性能的方法。我先在客户端抓了个包，然后尝试了惯用的三板斧。
1. **在Wireshark的Analyze→Expert Info→Notes菜单中看到图1的重传统计**，上万个包中只有7个需要重传，比例不算高。
    图1
    ![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307081557211.png)
2. **点击一个来自服务器的Ack查看往返时间**（RTT）。由图2底部可见，大概是78毫秒。我随机点击了很多个Ack，都差不多是这个值。
    图2
    ![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307081558058.png)

收集完这些信息就可以初步分析了：丢包不多，RTT也很稳定，但数据却传不快。难道是客户端的TCP发送窗口太小了吗？说到这里就需要补充点基础知识了：**决定客户端发送窗口的因素有两个，分别为网络上的拥塞窗口（CongestionWindow，缩写为cwnd）和服务器上的接收窗口**。后者与本案例无关（已经大到可以忽略），而且在本书的《技术与工龄》一文中详细介绍了，这里就不再赘述。

本文要讲的是更有技术含量的cwnd，学过TCP协议的工程师都知道，cwnd的增长方式是先“慢启动”，然后再进入“拥塞避免”。**前者起点低但能快速增长**；**后者起点高，但是每个RTT只能增加一个MSS**（Maximum Segment Size，表示一个TCP包所能携带的数据量）。在坐标轴中是这样表示cwnd的增长过程的，见图3。
图3
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307081559473.png)
了解完基础知识，我们再回头看看Wireshark里的cwnd。**选中一个发送窗口中最后的那个包，就可以看到它的“Bytes in flight”，它在本案例中就代表了cwnd的大小**。我随机选中了1970号包，从图4可见其cwnd为76020。
- 根据图3的理论，如果当时处于“拥塞避免”阶段，那下一个cwnd应该就是76020加上一个MSS（以太网中大概为1460字节），变成77480。
- 如果是在慢启动阶段，那就远远不止这么大。
    图4
    ![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307081603603.png)

然而再看图5，Wireshark中却显示下一个RTT（1974号包）的cwnd为76215。也就是说经历了一个RTT之后才增加了195个字节，远不如我们所期望的。我接着又往下看了几个RTT，还是一样的情况。这意味着**客户端的发送窗口增长非常慢，所以传输效率就很低**。
图5
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307081603699.png)
性能差的原因终于找到了，但是客户端为什么会有这种诡异的表现呢？更神奇的是，同样的客户端发数据给其他服务器就没有这个问题，因此我们还不能把问题根源定位到客户端上。当你百思不得其解的时候，最好是去代码里看看cwnd的计算方式是不是有问题。

但如果没有客户端的代码，或者根本看不懂代码呢？熟读RFC的优势就体现出来了。在RFC5681中讲到了多种cwnd的计算方式，其中有一种是这样的：Another common formula that a TCP MAY use to update cwnd during congestion avoidance is given inequation(3): `cwnd+=MSS*MSS/cwnd(3)`

单看公式不太容易理解，用人话解释一下就是这样的：假如客户端的当前cwnd大小为n个MSS，它就会在一个窗口里发出去n个包，然后期望收到n个Ack。每收到1个Ack它就把cwnd增加 `MSS*MSS/cwnd` ，于是收到n个Ack之后就总共增加了 `MSS*(n*MSS/cwnd)` 。由于cwnd等于n个MSS，所以括号里的 `(n*MSS/cwnd)` 大约等于1，从而实现了每经过1个RTT就增加1个MSS的目的。

**假如客户端采用的就是这个算法，那的确是可能导致cwnd增长过慢的**，因为它只有在收到n个Ack的情况下才能按预期增长，而**世界上并非每台服务器都是收到n个数据包就回复n个Ack的**。我实验室中的Linux服务器就是累计收到两个数据包才Ack一次，这就意味着客户端每经过1个RTT只能增长1/2个MSS。可是即使这样也比我手头的案例好得多啊，195个字节还不到1/7个MSS呢。**这说明Ack的频率非常之低**，在Wireshark里也很容易证实这一点。

于是这个问题就转换为如何提高服务器的Ack频率了。几番搜索之后，**我们发现这台服务器的网卡上启用了Large Receive Offload（LRO），会积累多个TCP包再集中处理，因此Ack数就比别的服务器少很多**，这也解释了为什么其他服务器没有性能问题。后来系统管理员用ethtool命令关闭LRO就把问题解决掉了。

总结下来，本案例中的客户端采用了一种不太科学的cwnd算法，服务器上又启用了LRO。两者分开工作的时候都没有问题，但是配合起来就会导致cwnd上升过慢，从而极大地影响了性能。这类问题如果没有Wireshark，我们估计都无法定位；而如果不熟读RFC，就算用上Wireshark也不知道如何解决。就像武侠小说里的内力和剑法一样，两者都很重要。
