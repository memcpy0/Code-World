赶紧搜索一下 Isilon，才知道是我司最近收购的 NAS，以性能卓越著称。是什么问题能让实施团队卡住好几天呢？看看时钟已经是凌晨 2 点了，便让现场的工程师先把网络包传上来再说。

一路疾驶到办公室，网络包也已经上传完毕。我用 Wireshark 粗略一看，发现很多包发生了重传（Retransmission），而且还有大量乱序（Out-Of-Order）。下面是 Wireshark 的分析结果。
重传（见图 1）：
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131634835.png)
图 1
乱序（见图 2）：
图 2
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131634520.png)
我的第一反应便是**乱序导致了重传**（乱序不代表丢包了），从而影响了性能。乱序为什么会导致重传呢？本书的 TCP 相关内容其实已有详细解释，下面再简单介绍一下。

在正常情况下，网络包到达接收方时的 Seq 号应该是顺序的，比如在每个包长度为 1460 的情况下，Seq 号可能是这样的：1460，2920，4380……因此接收方能算出下一个包的 Seq 号应该是什么。比如 4380 之后应该是 4380+1460=5840，
**假如收到的不是 5840，接收方就知道包序乱了**。这时它应该回复一个包给发送方，说“我要的是 5840（即 Ack 5840）”。如果接下来收到的包仍然不是 5480，那接收方就再回复一次“我要的是 5840”。

而==对于发送方来说，持续收到“我要的是 5840”可能意味着 5840 跑到其他
包后面了，也可能意味着 5840 已经丢失==。RFC 里这样定义：如果发送方收到 3个及以上**重复的**“我要的是×”，即可认为包×已经丢失，应当启动快速重传。图
3 演示了这个过程。
图 3
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131638661.png)
最终接收方会收到两个一样的 Seq=5480，即乱序了的原始包，还有一个重传
包。其中第二个到达的包相当于浪费了。

我在 Wireshark 上随机挑出几个重传包，发现方向都是从 Isilon 到 Windows 的，恰好符合读性能差的症状。分析到这里，我仿佛看到一丝曙光。一般来说，乱序可能是由发送方或者网络设备导致的，我还应该在发送方抓包进一步调查。但因为手头上只有在接收方抓的包，所以只能到了现场再说了。在赶往机场的路上，我草拟了一个计划。
1. 把 Isilon 和 Windows 客户端连到**同一台空闲的交换机**，尽量排除网络设备的影响。
2. Isilon 和其他服务器一样，应该**有类似 NIC teaming 的功能**。根据我的经验，==乱序有时候就是由 teaming 导致的，可以尝试关闭==。我不久前==还碰到过 Large Segment Offload（LSO）导致的乱序==，也是一个考虑因素。
3. 实在不行，就在 Isilon 和 Windows 上同时抓包，两者一对比便能发现很多问题。

到了北京已经是下午了。和几位来自中国香港、美国、和日本的工程师边吃边聊。原来他们这几天做过很多方面的尝试，包括我计划中的第 1 步，但是性能没有任何改变。Windows 客户端也换过几台，但结果都差不多。目前来看网络设备和客户端都不是瓶颈，估计原因就出在 Isilon 上了。也许明天关闭 Isilon 上的异NIC teaming 和 LSO，问题就解决了吧？这个时候我还是挺乐观的。

第二天一大早便赶到了××电视台的新大楼，比约定时间早了 3 小时。这是我第一次体会到现场工程师的辛苦—所有操作都要等待客户审批，搭个测试环境就花了半天时间；而且五六个人只能共用一台电脑，我在操作的时候其他工程师就只能等着；最可怕的是机房里的冷气，待了几个小时之后实在招架不住。

幸好一切都在按计划进行。我们终于在 Isilon 上找到 Large Segment Offload和 NIC teaming 的开关，并满怀希望地关闭了它们。当我启动测试脚本的时候，几位饱受折磨的现场工程师都凑过来看……可惜结果令人大跌眼镜—读性能比之前还差！我顿时觉得非常尴尬，对着等待我下一步建议的同事们，只能说先抓个包看看吧。这一抓包更是意外，居然看不到乱序的包了！可见我之前的猜测没有错，**乱序是由 NIC teaming 或者 LSO 导致的**。但为什么消除了乱序之后性能没有改善呢？再看看重传率，果然还是很高。

到这里只剩下一个解释了—**重传并非乱序引起的**，也就是说从一开始就走错
方向。我不得不一个人坐到角落里，重新研究昨天拿到的网络包。当我逐个检查乱序的包时，果然看到了一个很有趣的现象。如图 4 所示，虽然乱序的包很多，但只是相邻两个包的颠倒，因此接收方只发出了 1 个“我要的是×”，而不会凑满 3 个以上相同的“我要的是×”来触发重传。这就解释了为什么重传不是由乱序导致的。
图 4
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131642480.png)
举个更通俗的例子，当序号为 1、2、3、4、5、6 的一系列包到达接收方时，如果次序乱成了 2、1、4、3、6、5，是不会触发快速重传的；但如果乱成 2、3、4、5、6、1，就会导致重传。

再分析消除**乱序后在接收方抓到的网络包**，现象就更加有趣了。如图 5 所示，接收方明明收到了 Seq 20440（Frame No. 3），但它竟然发送了 4 个“Ack 20440”给发送方，从而促使发送方重传了 Seq 20440 （Frame No. 13）。
图 5
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131643685.png)
这个现象实在太“不科学”了。按理说这个包是在接收方抓的，Wireshark 上也已
经显示了“Seq 20440”，就意味着接收方已经收到。为什么还会连发 4 个 Dup Ack 呢？

我百思不得其解，不过已经隐约感觉到希望—只要解开这个谜团，问题或许就能解决了。机房里强劲的冷气让我有些分神，于是我独自踱到走廊上，从头开始分析。

我回忆起 RFC 中关于快速重传的描述：“当接收方收到比期望值大的 Seq 时，就要向发送方 Ack 它期望的 Seq 值……”根据这个理论，**难道接收方在收到 20440之前，已经收到了 21900、23360、24820 和 26280 这 4 个包**？从 Wireshark 里看20440 明明是排在这 4 个包前面的！

会不会是 20440 本身的 checksum 有问题，被接收方抛弃了呢？再看看图 5 中最后两个包，重传的 Seq 20440（Frame No. 13）到达接收方之前，接收方已经回复了“Ack 27740”（Frame No. 12），这表明接收方收到了 27740 之前的所有包，包括 20440。也就是说，20440 真的是被移到 26280 后面了，而不是因为 checksum 无效被抛弃。

那是什么因素导致接收方把 20440 移到 26280 后面呢？目前我不得而知，但
TCP/IP 是分层协作的，也许是网络层把包交给 TCP 层时打乱了。

分析到这里，可以肯定**重传的根本原因就是接收方自身的乱序，而网络设备和 Isilon 都被冤枉了**。这是我第一次看到此类现象，不但颠覆了我昨天的分析结
果，而且难以说服现场工程师和客户。他们已经测试了 7 台客户端，但结果都是
一样的，难不成 7 台都出了同样的问题？这概率低得令人难以置信。接下来就是
一场场辩论，电视台请来了他们的网络专家，希望说服我进一步检查 Isilon。我无
法向他解释为何所有客户端都有同样的问题，他也不能反驳 Wireshark 上显示的
证据。一直拉锯到夜里 12 点都没有吃上饭，一位同事已经出现了低血糖症状。还好最后查到一个重要信息，原来那 7 台客户端都是用同一张 ghost 盘安装的，客户终于让步，答应明天新装 7 台客户端供我们测试。但同时也有一个要求，明早必须提供一个官方的分析报告，证明的确是客户端导致的问题。

草草吃完晚饭，已经是凌晨 1 点。酒店非常贴心，为我准备好了巧克力，拆好拖鞋，甚至掀好了被子，可惜这些我都没有机会享受。等写完分析报告，已经
到了凌晨 3 点半。没睡下多久，morning call 又来了……再次感叹现场工程师的辛苦，这只是我第三个晚上没睡好，而他们估计已经有一周了。我睡眼惺忪地到了电视台门口，现场工程师手脚麻利，很快就搭好新的环境。到早上 10 点钟我们又一次启动测试脚本，这一次每台的读性能都达到 100MB/s 以上，大大超过了客户 80MB/s的预期。现场的工程师异常兴奋，给测试结果拍照、截屏，甚至拍了一段视频。

他们为这个项目压抑太久了，需要好好庆祝。而我也背起笔记本，向这栋造型诡异的建筑、向这个奇怪的问题告别，匆匆赶往首都机场。家里还有发烧的老婆，没搬完的家……