一位澳洲客户的文件服务器，它同时为多台 Linux 应用服务器提供 NFS访问。系统在实施阶段非常顺利，于是便择日上线了。不幸的是到了生产环境中，应用服务器访问文件时偶尔会卡一下，而且这症状的出现是不定时的、稍纵即逝的。谁也不知道接下来是什么时候，发生在哪台应用服务器上。经验丰富的系统管理员已经检查过应用服务器、文件服务器和网络设备的所有日志，可惜没有发现有价值的信息。

老油条的工程师都知道，这类问题是最“令人讨厌”的，因为既无报错信息，也不知道何时会重现，根本无从入手。大家宁愿处理丢数据或者宕机的紧急事故
也不愿意去接手这类问题。可怜的系统管理员不时被他的用户埋怨，然后再把压
力转移到售后工程师身上。一线的售后工程师扛了一个礼拜没有解决，只好升级
到二线。二线工程师撑了一个礼拜也没有收获，最终找到了我。大家可以想象当时那位系统管理员已经有多么沮丧。

问题到了我这里就没法再升级了，只能硬着头皮接下来。我是这样分析该症状的。
1. 访问文件时感到卡，可能是文件服务器负载过重，导致了响应慢；也可能是网络拥塞，发生了连续多次的重传。
2. 虽然无法预测问题发生的时间，但如果在业务繁忙时抓个网络包，应该多少能看到一些端倪。

当我把这个想法告诉系统管理员时，得到的回答却让我颇感意外：“存储上的网络包我已经抓过了，分析下来一点问题都没有。”—在我以往接触过的客户中，不要说分析网络包了，很多人连抓包都不会。这个分析可靠吗？还没等我开口，他似乎看透了我的心思，“网络包上传到 FTP 了，你也分析一下吧。”用 Wireshark 打开网络包之后，我习惯性地试了“性能问题三板斧”。
1. 单击 Statistics-->Summary。从 Avg.MBit/sec 看到，那段时间的流量不高，所以该存储的负担似乎并不重（见图 1）。
    图 1
    ![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131703046.png)
    单击 Statistics-->Service Response Time-->ONC-RPC-->Program:NFS Version:3--> Create Stat，可以看到各项操作的 Service Response Time 都不错（见图 2），这进一步说明该存储并没有过载。
    图 2
    ![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131724626.png)
3. 单击 Analyze-->Expert Info Composite，从 Error 和 Warning 里都没有看到报错，这说明网络没有问题（见图 3）。假如有重传、乱序之类的现象，应该能在这个窗口里看到。
    图 3
    ![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131725723.png)

分析结果让我有些失望—这个系统看起来如此健康，完全不像是会卡的样子，接下来该怎么处理？看来一定要在出问题的时刻抓到包，舍此之外，别无他途了。我小心翼翼地给管理员写了一封邮件，把分析结果详细地告诉他，并且提出再次抓包的请求。

等待回复时很是忐忑，因为有可能收到一堆抱怨，没想到等来的竟是一个惊喜—他表示遇到一位懂 Wireshark 的合作者非常愉快，并且准备写一个程序来抓到我需要的包。这个程序会不停地打开文件，当出现卡的症状时，记录时间点并且自动停止抓包。碰到如此讲道理又懂技术的客户，简直让人如沐春风。好消息接踵而至，几天后网络包真的抓到了，还记录了出问题的时间点。

我满怀希望地又试了三板斧，预感这次一定能看到某些迹象，比如特别长的Service Response Time 之类的。没想到一番忙活之后，竟然和之前的分析结果一模一样—什么迹象都没看到。

不会是漏抓了吧？考虑到这位管理员的表现非常靠谱，应该不至于犯这样的小错误，我宁愿相信是自己看得不够仔细。就在此时，我又收到一封邮件。原来他也分析完了，一样没有发现什么问题。同时也强调自己没有漏抓，相信问题一定就隐藏在包里。

我不由得会心一笑：好默契的回复！今天算是遇到对手了，这是我工作这么多年来第一次碰到如此厉害的角色。既然三板斧没有用，只能采用笨办法了。我先根据问题发生的时间点过滤出前后 2 秒钟的所有包，然后逐个检查。这下果然看到一个意想不到的包：如图 4 中的包号 440354 所示，NFS 服务器 172.16.2.80
给客户端 172.16.2.102 发了一个 Portmap 请求，咨询其 NLM 进程的端口号。更异常的是这个请求竟然没有得到回复。
图 4
![](https://image-1307616428.cos.ap-beijing.myqcloud.com/Obsidian/202307131727749.png)

==NLM 我是听说过的，是 Network Lock Manager 的简称。客户端用它来锁定服
务器上的文件，从而避免和其他客户端发生访问冲突==。一般都是由客户端查询服务器的 NLM 端口，这种反方向的状况我还是第一次见到。这个 Portmap 请求出现在这里虽然有点突兀，不过似乎可以忽略，因为我想不出它跟访问文件卡有什么联系。

遍历了所有包之后，仍然一无所获。我几乎想放弃了，沮丧的感觉就像交卷时还解不出最后一道大题。但要真正放弃又不甘心，毕竟投入了这么多时间了，而且也对不起这么配合的系统管理员。我之所以至今对这个案例如此印象深刻，就是因为工作以来第一次感觉问题这么棘手。

纠结了一天之后，我还是决定从头再来，这次要更细致地分析每一个包。既然目前唯一发现的异常就是那个关于 NLM 的 Portmap 查询，那就从它开始吧。我收集了一些资料，重温了一遍 NLM 的工作原理（虽然我以前懂过，但细节性的东西一段时间没有接触，是很容易忘记的），然后把 NLM 工作过程总结如下。
1. 客户端甲→NLM_LOCK_MSG request→NFS 服务器（甲尝试锁定一个文件）
2. 客户端甲←NLM_LOCK_RES granted ←NFS 服务器（服务器同意了这个锁定）
3. 客户端乙→NLM_LOCK_MSG request→NFS 服务器（乙尝试锁定同一个文件）
4. 客户端乙←NLM_LOCK_RES blocked←NFS 服务器（因为该文件已经被甲锁定，所以服务器让乙等着）
5. 客户端甲→NLM_UNLOCK_MSGrequest→NFS 服务器（甲尝试释放锁）
6. 客户端甲←NLM_UNLOCK_RES granted←NFS 服务器（服务器同意释放）
7. 客户端乙←NLM_GRANTED_MSG←NFS 服务器（服务器主动把锁给了乙）
8. 客户端乙→NLM_GRANTED_RES accept→NFS 服务器（乙接受了）

Wireshark 里看到的那个 Portmap 请求，发生在上面的哪个步骤呢？应该在第三步和第四步之间。就在找到答案的一刹那，我恍然大悟，一下子知道问题出在哪了。
1. 第三步之后，==服务器要通过 Portmap 查询乙的 NLM 端口号（也就是那个诡异的包），得到回复后才能进入第四步==。
2. 假如查询端口号失败，则第四步无法进行，也就意味着服务器没有办法把锁给乙。
3. ==由于乙得不到锁，所以只能继续等到超时为止==。这对于应用程序来说，就是卡住了。
4. 该问题只发生在多个客户端同时访问同一文件的情况下，所以表现为偶发症状。
5. 乙没有响应 Portmap 查询，很可能是包被防火墙拦截了。

我来不及写邮件，就迫不及待地抓起电话，把分析结果告诉南半球的系统管理员。他也非常兴奋，很快就修改了防火墙设置，从此再也没有用户报告过卡的现象。

事情是否到此结束了呢？这个症状的确结束了。不过用户又反馈了另一个症状，这一次连 Wireshark 都无能为力，最后还是 Patrick 专门写了段脚本才解决的。