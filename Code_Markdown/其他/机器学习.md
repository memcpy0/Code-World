相异度（距离）：
- $r = 1$ ，曼哈顿距离（例如海明距离）
- $r = 2$ ，欧氏距离 $\displaystyle d(x,y)= \sqrt { \sum^n_{k=1}(x_y - y_k)^2 }$
- $r = \inf$ ，闵氏距离 $\displaystyle d(x,y)=(\sum^n_{k=1} | x_k - y_k|^r)^{\dfrac{1}{r}}$ 
广义平均：从上到下递增，$M_p = (\dfrac{1}{n} \sum^n_{k=1} x_k^p)^{\dfrac{1}{p}}$
- 最小值：$M_{-\inf} = \min$
- 调和平均：$M^{-1} = \dfrac{n}{ \dfrac{1}{x_1} + \dots + \dfrac{1}{x_n}}$ 
- 几何平均：$M_0 = \sqrt[n] { x_1 \dots x_n}$
- 算术平均：$M_1 = \dfrac{x_1 + \dots + x_n}{n}$
- 多项式平均：$M_2 = \sqrt {\dfrac{x_1^2 + \dots + x^2_n}{n}}$
- Cubic平均：$M_3 = \sqrt[3] {\dfrac{ x_1^3 + \dots + x_n^3} { n}}$
- 最大值：$M_{\inf} = \max$

K-means算法介绍：这一算法无法从数据中获得簇数量，簇的个数 $k$ 和样本数据是用户给定的，每个簇都可以通过簇中心来描述。

K-means算法的过程如下：
1. 随机将样本数据分为 $k$ 类 $C_1, \dots, C_k$ ，并计算出 $k$ 个初始质心 $c_1,\dots, c_k$ 。
2. 对数据集中的每个数据点，计算它们与每个质心的欧氏距离。
3. 将数据点分配到距离它们最近的簇中。
4. 对新的簇，重新计算簇中所有点的均值，并将其作为质心。
5. 接着循环2~4，直到簇中心和簇不再变化。

衡量聚类效果的是**误差平方和**：
$$SSE = \sum^k_{i=1} \sum_{x \in C_i} dist(c_i, x)^2$$

主成分分析：通过原有变量的线性组合来解释原有变量的大部分信息。
1. 对所有样本进行去中心化，一个样本 $x_i$ 有多个属性/特征，求均值就是求这些属性的均值。所有这些样本组成矩阵 $X$ 。
	$$x_i = x_i - \dfrac{1}{n} \sum^n_{j=1} x_j$$
2. 计算去中心化样本的协方差矩阵，即 $A= X^T X$ 。
$$\begin{bmatrix} cov(x_1, x_1) & cov(x_1, x_2) \\ cov(x_2, x_1) & cov(x_2, x_2) \end{bmatrix}$$
3. 对协方差矩阵进行特征值分解，求出特征值和单位特征向量。
	特征值分解：**求出 $|\lambda E - A|= 0$ 的每个根 $\lambda$ ，每个根都是矩阵 $A$ 的特征值**。求特征值的过程中可能需要对行列式进行行或列变换，只要一行或一列中只有一个非零元素，就可以将行列式展开——将该行/列各元素 $a_{ij}$ 分别乘以对应代数余子式 $A_{ij} = (-1)^{i+j} M_{ij}$ ，二阶行列式可以直接计算。
	方程组 $(\lambda E - A)x = 0$ 的每个非零解都是 $\lambda$ 对应的特征向量。矩阵 $|\lambda E - A|$ 化简为行最简阶梯型矩阵后，可能有多个自由变量，对自由变量可以依次取 $1, \dots, 0$ 、$0, 1,\dots, 0$ 、$0, 0, \dots, 1$ 。只有一个自由变量如 $x_i$ 时，可以将其他变量 $x_j$ 分别写成 $x_j = k_j x_i$ ， 而后 $[k_1, k_2, \dots, k_{i-1}, 1]$ 就是我们的特征向量。
4. 取出最大的 $m$ 个特征值对应的单位特征向量（$m$ 即降维后的维度数），组成特征向量矩阵 $W = (w_1, w_2, \dots, w_m)^T$ 。
5. 对去中心化样本中每个 $x_i$ ，将其转换为新的样本 $z_i = W^T x_i$ ，得到输出样本。

LDA是有监督数据降维方法，它将一个高维空间中的数据投影到一个低维空间（一条适当选择的直线上），且投影后要保证各类类内方差小、类间均值差别大，即同一类高维数据投影到低维空间中聚在一起，不同类别间相差较远。

我们把这个最佳投影向量称为 $w$ ，考虑其长度为1，只求其方向。样例 $x$ 到 $w$ 的投影即为 $y=w^T x$ 。

从二分类问题开始：假设有一组 $m$ 个 $d$ 维数据样本 $x_1, \dots, x_m$ ，分别属于集合 $X_0, X_1$ ，$|X_0|=n_0, |X_1|=n_1$ ，$m$ 个样本投影后产生的结果属于集合 $Y_0,Y_1$ ，即为 $Y_i = w^T X_i\ (i =0,1)$ 。

设 $\mu_i$ 为原样本均值：$$\mu_i = \dfrac{1}{n_i} \sum_{x \in X_i} x\ (i = 0, 1)$$
则投影后的样本均值为 $$\tilde { \mu_i} = \dfrac{1}{n_i} \sum_{y \in Y_i} y = \dfrac{1}{n_i} \sum_{x \in X_i} w^T x = w^T \mu_i\ (i = 0, 1)$$
恰好是原样本均值的投影。

设 $s_i^2$ 为原样本方差：$$s_i^2 = \dfrac{1}{n_i} \sum_{x \in X_i} (x - \mu_i)^2$$
则投影后样本的第 $i$ 类类内散度为：$$\tilde {s_i^2}= \sum_{y \in Y_i} (y - \tilde{\mu_i})^2$$
我们将 $\tilde {s_0^2} + \tilde {s_1^2}$ 称为投影后样本总体类内散度。Fisher线性可分性准则要求，在投影 $y=w^Tx$ 下，准则函数最大化：$$J(w) = \dfrac{ | \tilde{\mu_0} - \tilde{\mu_1} |^2}{ \tilde {s_0^2} + \tilde {s_1^2} }$$
将投影后样本均值之差展开： $$| \tilde{\mu_0} - \tilde{\mu_1} |^2 = | w^T \mu_0 - w^T\mu_1|^2 = w^T(\mu_0 - \mu_1)(\mu_0 - \mu_1)^T w$$
定义 $S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$ 为类间散度矩阵，可证明为对称半正定的。

定义原样本空间 $X_i$ 的类内散度矩阵为：$$\sum_i = \sum_{x\in X_i} (x - \mu_i) (x - \mu_i)^T$$
总体类内散度矩阵为 $S_w = \sum_0 + \sum_1$ ，可证明为对称半正定的。

则投影后第 $i$ 类类内散度如下：
$$\tilde {s_i^2}= \sum_{y \in Y_i} (y - \tilde{\mu_i})^2 
=\sum_{x\in X_i} (w^Tx - w^T\mu_i)^2 = \sum_{x\in X_i}w^T (x-\mu_i)(x-\mu_i)^T w = w^T \sum_i w$$
于是总体类内散度 $$\tilde {s_0^2} + \tilde {s_1^2} = w^T S_w w$$
从而 $J(w) = \dfrac{ w^T S_b w}{w^T S_w w}$ 。令 $w^T S_w w = c \ne 0$ ，运用拉格朗日乘子法：
$$L(w, \lambda) = -w^T S_b w + \lambda (w^T S_w w - c)$$
从而：$$\nabla_w L(w,\lambda) = \dfrac{\partial_{L(w, \lambda) }}{\partial_w} = -2S_bw + 2\lambda S_w w $$ 即：$$S_b w = \lambda S_w w$$
可知：$S_b w = (\mu_0 - \mu_1) (\mu_0 - \mu_1)^T w$ 中 $S_b w$ 与 $(\mu_0 - \mu_1)$ 是同向向量，而 $(\mu_0 - \mu_1)w=\lambda'$ 是标量。令 $S_b w = \lambda' (\mu_0 - \mu_1)$ 。

于是 $$S_b w = \lambda' (\mu_0 - \mu_1) = \lambda S_w w$$ $$w = \lambda^{-1}\lambda' S_w^{-1}(\mu_0 - \mu_1)$$ 于是最佳方向向量为 $w^* = S_w^{-1}(\mu_0 - \mu_1)$。$J(w)$ 最大值为 $(\mu_0 - \mu_1)^T S_w^{-1} (\mu_0 - \mu_1)$ 。最佳投影变换 $y_0 = (\mu_0 - \mu_1)^T S_w^{-1} x$ 。


